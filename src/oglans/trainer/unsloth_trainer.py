"""
Unsloth DPO Trainer Wrapper (v4.0 OG-LANS Final)
é›†æˆ DS-CNS åŠ¨æ€è´Ÿé‡‡æ ·ã€SCV è¯­ä¹‰æ ¡éªŒå’Œ CGA æ¢¯åº¦æ”¾å¤§çš„ DPO è®­ç»ƒå™¨

å­¦æœ¯åˆ›æ–°ç‚¹:
1. LANS: åŸºäºæŸå¤±çš„è‡ªé€‚åº”è´Ÿé‡‡æ ·è°ƒåº¦
2. CGA: å¯¹æ¯”æ¢¯åº¦æ”¾å¤§æœºåˆ¶ (Contrastive Gradient Amplification)
3. å¤šç²’åº¦æ‰°åŠ¨ç­–ç•¥é›†æˆ
"""

import unsloth
from unsloth import FastLanguageModel, PatchDPOTrainer

# Patch DPOTrainer globally before importing TRL to ensure patching works for subclasses
PatchDPOTrainer()

import torch
import transformers
from transformers import TrainerCallback, TrainerState, TrainerControl
import json
import os
import logging
import time
import hashlib
from collections import OrderedDict
# Features, Value ç”¨äºæ˜¾å¼å®šä¹‰æ•°æ®é›† Schema
from datasets import Dataset, Features, Value
from trl import DPOTrainer, DPOConfig
try:
    from trl import DPODataCollatorWithPadding
except ImportError:
    # Fallback for older trl versions or when not exported
    from dataclasses import dataclass
    from transformers import PreTrainedTokenizerBase, DataCollatorForSeq2Seq
    from typing import Any, Dict, List, Optional, Union

    @dataclass
    class DPODataCollatorWithPadding:
        """
        DPODataCollatorWithPadding å…¼å®¹æ€§å›é€€å®ç°
        """
        tokenizer: PreTrainedTokenizerBase
        padding: Union[bool, str] = True
        max_length: Optional[int] = None
        pad_to_multiple_of: Optional[int] = None
        label_pad_token_id: int = -100

        def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
            # ä½¿ç”¨ DataCollatorForSeq2Seq è¾…åŠ©å¡«å……
            base_collator = DataCollatorForSeq2Seq(
                tokenizer=self.tokenizer,
                padding=self.padding,
                max_length=self.max_length,
                pad_to_multiple_of=self.pad_to_multiple_of,
                label_pad_token_id=self.label_pad_token_id
            )

            batch = {}

            # å®šä¹‰éœ€è¦å¤„ç†çš„ DPO å­—æ®µç»„
            # æ ¼å¼: {ç›®æ ‡å‰ç¼€: {åŸå§‹å­—æ®µå: ä¸´æ—¶å­—æ®µå}}
            groups = {
                "chosen": {
                    "chosen_input_ids": "input_ids",
                    "chosen_attention_mask": "attention_mask",
                    "chosen_labels": "labels"
                },
                "rejected": {
                    "rejected_input_ids": "input_ids",
                    "rejected_attention_mask": "attention_mask",
                    "rejected_labels": "labels"
                },
                "prompt": {
                    "prompt_input_ids": "input_ids",
                    "prompt_attention_mask": "attention_mask"
                }
            }

            for group_name, mapping in groups.items():
                # æå–å­ batch
                sub_features = []
                for f in features:
                    item = {}
                    has_data = False
                    for orig, temp in mapping.items():
                        if orig in f:
                            item[temp] = f[orig]
                            has_data = True
                    if has_data:
                        sub_features.append(item)

                if not sub_features:
                    continue

                # å¡«å……
                padded = base_collator(sub_features)

                # è¿˜åŸå­—æ®µåå¹¶åˆå¹¶åˆ° batch
                for orig, temp in mapping.items():
                    if temp in padded:
                        batch[orig] = padded[temp]

            # å¤åˆ¶å…¶ä»–é Tensor å­—æ®µ (å¦‚ prompt æ–‡æœ¬ç­‰)
            if features:
                for k in features[0].keys():
                    if k not in batch:
                        batch[k] = [f[k] for f in features]

            return batch

from ..utils.ds_cns import DSCNSampler, LANSScheduler
from ..utils.scv import SemanticConsistencyVerifier
from ..data.prompt_builder import ChinesePromptBuilder, build_inference_prompt
import random
import numpy as np
from typing import Optional, Dict, Any, List, Union, Tuple
from torch.utils.data import IterableDataset
from torch.utils.tensorboard import SummaryWriter

logger = logging.getLogger("OGLANS")


# ============================================================================
# CGA-Enhanced DPO Trainer: å¯¹æ¯”æ¢¯åº¦æ”¾å¤§æœºåˆ¶
# ============================================================================

class CGADPOTrainer(DPOTrainer):
    """
    CGA-Enhanced DPO Trainer (OG-LANS æ ¸å¿ƒåˆ›æ–°ç»„ä»¶)
    
    Contrastive Gradient Amplification (CGA) å¯¹æ¯”æ¢¯åº¦æ”¾å¤§:
    - æ ¹æ®æ¨¡å‹å½“å‰èƒ½åŠ›åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡
    - èƒ½åŠ›å¼±æ—¶æ”¾å¤§æ¢¯åº¦ï¼Œå¢å¼ºå¯¹æ¯”å­¦ä¹ ä¿¡å·
    - èƒ½åŠ›å¼ºæ—¶æ¢å¤æ­£å¸¸ï¼Œé¿å…è¿‡æ‹Ÿåˆ
    
    æ•°å­¦å…¬å¼: 
        loss_weighted = loss Ã— (1 + Î²_cga Ã— (1 - C))
    å…¶ä¸­ C æ˜¯å½“å‰èƒ½åŠ›ä¼°è®¡å€¼ (0~1)
    """
    
    def __init__(
        self, 
        *args, 
        lans_scheduler: Optional[LANSScheduler] = None,
        rpo_alpha: float = 0.0,
        rpo_warmup_steps: int = 0,
        aux_log_interval: int = 50,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.lans_scheduler = lans_scheduler
        self.rpo_alpha = max(0.0, float(rpo_alpha))
        self.rpo_warmup_steps = max(0, int(rpo_warmup_steps))
        self.aux_log_interval = max(1, int(aux_log_interval))
        self._cga_applied_count = 0
        self._rpo_warning_emitted = False
        self._last_aux_metrics: Dict[str, float] = {}
        
        if self.lans_scheduler is not None:
            logger.info(
                f"ğŸš€ CGADPOTrainer åˆå§‹åŒ–: "
                f"CGA_enabled={self.lans_scheduler.use_cga}, "
                f"CGA_beta={self.lans_scheduler.cga_beta}"
            )
        if self.rpo_alpha > 0:
            logger.info(
                f"ğŸ¯ RPO æ··åˆæŸå¤±å·²å¯ç”¨: alpha={self.rpo_alpha:.4f}, "
                f"warmup_steps={self.rpo_warmup_steps}"
            )

    def _compute_rpo_weight(self) -> float:
        """RPO æƒé‡è°ƒåº¦ï¼šé¢„çƒ­åè¾¾åˆ°ç›®æ ‡ alphaã€‚"""
        if self.rpo_alpha <= 0:
            return 0.0
        if self.rpo_warmup_steps <= 0:
            return self.rpo_alpha
        step = int(getattr(self.state, "global_step", 0))
        scale = min(1.0, max(0.0, step / max(self.rpo_warmup_steps, 1)))
        return self.rpo_alpha * scale

    @staticmethod
    def _compute_chosen_sft_loss(
        model: Any,
        inputs: Dict[str, Union[torch.Tensor, Any]],
    ) -> Optional[torch.Tensor]:
        """è®¡ç®— chosen åºåˆ—çš„ NLLï¼ˆSFT anchorï¼‰ã€‚"""
        chosen_input_ids = inputs.get("chosen_input_ids")
        chosen_attention_mask = inputs.get("chosen_attention_mask")
        chosen_labels = inputs.get("chosen_labels")

        if chosen_input_ids is None or chosen_labels is None:
            return None
        if not isinstance(chosen_labels, torch.Tensor):
            return None
        if not torch.any(chosen_labels != -100):
            return None

        outputs = model(
            input_ids=chosen_input_ids,
            attention_mask=chosen_attention_mask,
            labels=chosen_labels,
            use_cache=False,
            return_dict=True,
        )
        sft_loss = getattr(outputs, "loss", None)
        if sft_loss is None:
            return None
        if not torch.isfinite(sft_loss):
            return None
        return sft_loss

    def get_aux_metrics_snapshot(self) -> Dict[str, float]:
        return dict(self._last_aux_metrics)
    
    def compute_loss(
        self,
        model: Any,
        inputs: Dict[str, Union[torch.Tensor, Any]],
        return_outputs: bool = False,
        num_items_in_batch: Optional[int] = None,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, Any]]]:
        """
        é‡å†™æŸå¤±è®¡ç®—æ–¹æ³•ï¼Œåº”ç”¨ CGA æ¢¯åº¦æ”¾å¤§
        
        è¿™æ˜¯ OG-LANS ä¸ DA-DPO/Hard Negative DPO çš„æ ¸å¿ƒå·®å¼‚:
        - DA-DPO: ä½¿ç”¨ VLM ç½®ä¿¡åº¦è°ƒæ•´é‡‡æ ·
        - Hard Neg DPO: ä½¿ç”¨éªŒè¯å™¨ç­›é€‰æ ·æœ¬
        - OG-LANS: ä½¿ç”¨æœ¬ä½“å›¾è·ç¦» + CGA åŠ¨æ€æ¢¯åº¦æ”¾å¤§
        """
        # è°ƒç”¨çˆ¶ç±»è®¡ç®—åŸå§‹æŸå¤±
        if return_outputs:
            loss, outputs = super().compute_loss(
                model, inputs, return_outputs=True, num_items_in_batch=num_items_in_batch
            )
        else:
            loss = super().compute_loss(
                model, inputs, return_outputs=False, num_items_in_batch=num_items_in_batch
            )
            outputs = None

        pref_loss_raw = loss
        cga_weight = 1.0

        # åº”ç”¨ CGA æ¢¯åº¦æ”¾å¤§ï¼ˆåªä½œç”¨äºåå¥½æŸå¤±é¡¹ï¼‰
        if self.lans_scheduler is not None and self.lans_scheduler.use_cga:
            cga_weight = self.lans_scheduler.cga_weight
            if self.lans_scheduler._step_count > self.lans_scheduler.warmup_steps:
                pref_loss_raw = pref_loss_raw * cga_weight
                self._cga_applied_count += 1
                if self._cga_applied_count % 100 == 0:
                    logger.debug(
                        f"CGA æ¢¯åº¦æ”¾å¤§: weight={cga_weight:.4f}, "
                        f"competence={self.lans_scheduler.competence:.4f}"
                    )

        # RPO: åå¥½æŸå¤± + alpha * chosen-SFT é”šå®šé¡¹
        rpo_weight = self._compute_rpo_weight()
        sft_loss: Optional[torch.Tensor] = None
        if rpo_weight > 0.0:
            try:
                sft_loss = self._compute_chosen_sft_loss(model, inputs)
            except Exception as exc:
                if not self._rpo_warning_emitted:
                    logger.warning(f"RPO è®¡ç®—å¤±è´¥ï¼Œå·²é€€åŒ–ä¸ºçº¯ DPO/IPO: {exc}")
                    self._rpo_warning_emitted = True

        final_loss = pref_loss_raw
        if sft_loss is not None and rpo_weight > 0.0:
            final_loss = pref_loss_raw + rpo_weight * sft_loss

        self._last_aux_metrics = {
            "pref_loss_raw": float(loss.detach().float().item()),
            "pref_loss_weighted": float(pref_loss_raw.detach().float().item()),
            "cga_weight": float(cga_weight),
            "rpo_weight": float(rpo_weight),
            "rpo_sft_loss": float(sft_loss.detach().float().item()) if sft_loss is not None else 0.0,
            "combined_loss": float(final_loss.detach().float().item()),
        }
        if getattr(self.state, "global_step", 0) % self.aux_log_interval == 0:
            logger.debug(
                "loss_components: "
                f"pref={self._last_aux_metrics['pref_loss_weighted']:.4f}, "
                f"rpo_w={self._last_aux_metrics['rpo_weight']:.4f}, "
                f"sft={self._last_aux_metrics['rpo_sft_loss']:.4f}, "
                f"combined={self._last_aux_metrics['combined_loss']:.4f}"
            )

        if return_outputs:
            return final_loss, outputs
        return final_loss


# ============================================================================
# LANS Callback: è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°èƒ½åŠ›è¯„ä¼°
# ============================================================================

class LANSCallback(TrainerCallback):
    """
    Loss-Aware Adaptive Negative Sampling Callback

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ¯ä¸ª Step ç»“æŸæ—¶æ›´æ–°èƒ½åŠ›å€¼ (Competence)
    2. æ¯ä¸ª Epoch å¼€å§‹æ—¶æŒ‰é…ç½®åˆ·æ–°è´Ÿæ ·æœ¬ï¼ˆåŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼‰
    3. è®°å½•ç¬æ—¶ç­–ç•¥åˆ†å¸ƒåˆ° TensorBoard
    """

    def __init__(
        self,
        lans_scheduler: LANSScheduler,
        log_interval: int = 10,
        lans_sampler: Optional['LANSNegativeSampler'] = None,
        lans_dataset: Optional['LANSIterableDataset'] = None,
        logging_dir: Optional[str] = None,
        trainer_ref: Optional[Any] = None,  # ã€æ–°å¢ã€‘Trainer å¼•ç”¨ï¼Œç”¨äºåŠ¨æ€æ›´æ–°æ•°æ®é›†
        base_samples: Optional[List[Dict]] = None,  # ã€æ–°å¢ã€‘åŸå§‹æ ·æœ¬æ•°æ®
        tokenizer: Optional[Any] = None,  # ã€æ–°å¢ã€‘Tokenizer å¼•ç”¨
        refresh_start_epoch: int = 1,
        refresh_log_interval: int = 100,
        refresh_log_seconds: float = 30.0,
    ):
        self.lans_scheduler = lans_scheduler
        self.log_interval = log_interval
        self.lans_sampler = lans_sampler
        self.lans_dataset = lans_dataset
        self._global_step = 0
        self._writer = SummaryWriter(log_dir=logging_dir) if logging_dir else None
        self.trainer_ref = trainer_ref
        self.base_samples = base_samples
        self.tokenizer = tokenizer
        self._current_epoch = -1  # è·Ÿè¸ªå½“å‰ Epoch
        self.refresh_start_epoch = max(0, int(refresh_start_epoch))
        self.refresh_log_interval = max(1, int(refresh_log_interval))
        self.refresh_log_seconds = max(5.0, float(refresh_log_seconds))
        self._refresh_warning_emitted = False

    def on_epoch_begin(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """
        Epoch å¼€å§‹æ—¶åŠ¨æ€é‡æ–°ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼‰

        åŸºäºå½“å‰ Competence åˆ·æ–°æ•´ä¸ªæ•°æ®é›†çš„ rejected å­—æ®µï¼Œ
        å®ç°çœŸæ­£çš„è¯¾ç¨‹å­¦ä¹ ï¼šéšç€æ¨¡å‹èƒ½åŠ›æå‡ï¼Œè´Ÿæ ·æœ¬éš¾åº¦é€æ¸å¢åŠ ã€‚
        """
        current_epoch = int(state.epoch) if state.epoch else 0

        # é¿å…é‡å¤è§¦å‘ï¼ˆæŸäº› Trainer å®ç°å¯èƒ½å¤šæ¬¡è°ƒç”¨ï¼‰
        if current_epoch == self._current_epoch:
            return
        self._current_epoch = current_epoch

        if self.lans_sampler is not None:
            self.lans_sampler.set_epoch(current_epoch)

        current_comp = self.lans_scheduler.competence
        current_threshold = self.lans_scheduler.current_threshold

        logger.info(
            f"ğŸ“… Epoch {current_epoch} å¼€å§‹: "
            f"èƒ½åŠ›å€¼={current_comp:.4f}, é˜ˆå€¼={current_threshold:.2f}"
        )

        if current_epoch < self.refresh_start_epoch:
            logger.info(
                f"â­ï¸ Epoch {current_epoch}: è·³è¿‡è´Ÿæ ·æœ¬åˆ·æ–° "
                f"(refresh_start_epoch={self.refresh_start_epoch})ï¼Œæ²¿ç”¨åˆå§‹è´Ÿæ ·æœ¬"
            )
            return

        # ã€æ ¸å¿ƒã€‘åŠ¨æ€é‡æ–°ç”Ÿæˆè´Ÿæ ·æœ¬
        if self.trainer_ref is not None and self.base_samples is not None and self.lans_sampler is not None:
            logger.info(f"ğŸ”„ Epoch {current_epoch}: åŸºäºå½“å‰èƒ½åŠ›å€¼é‡æ–°ç”Ÿæˆè´Ÿæ ·æœ¬...")
            refresh_start_ts = time.perf_counter()

            # ã€ä¿®å¤ã€‘æ¸…ç©ºæ»‘åŠ¨çª—å£ï¼Œç¡®ä¿ç»Ÿè®¡ä»…åæ˜ å½“å‰ Epoch çš„ç­–ç•¥åˆ†å¸ƒ
            if hasattr(self.lans_scheduler, '_recent_strategies'):
                self.lans_scheduler._recent_strategies = []

            # é‡æ–°ç”Ÿæˆæ‰€æœ‰æ ·æœ¬çš„ rejected
            new_prompts, new_chosens, new_rejecteds = [], [], []
            new_texts, new_event_types = [], []

            total_samples = len(self.base_samples)
            last_log_ts = refresh_start_ts
            for idx, sample in enumerate(self.base_samples, start=1):
                result = self.lans_sampler.generate_rejected(sample)
                new_prompts.append(result["prompt"])
                new_chosens.append(result["chosen"])
                new_rejecteds.append(result["rejected"])
                new_texts.append(sample.get("text", ""))
                new_event_types.append(sample.get("event_types", []))

                now = time.perf_counter()
                should_log = (
                    idx % self.refresh_log_interval == 0
                    or idx == total_samples
                    or (now - last_log_ts) >= self.refresh_log_seconds
                )
                if should_log:
                    elapsed = max(time.perf_counter() - refresh_start_ts, 1e-6)
                    speed = idx / elapsed
                    eta = max(total_samples - idx, 0) / max(speed, 1e-6)
                    logger.info(
                        f"   â³ Epoch {current_epoch} è´Ÿæ ·æœ¬åˆ·æ–°è¿›åº¦: "
                        f"{idx}/{total_samples} ({idx / max(total_samples, 1):.1%}), "
                        f"{speed:.2f} samples/s, ETA {eta:.1f}s"
                    )
                    last_log_ts = now

            # åˆ›å»ºæ–°æ•°æ®é›†
            new_dataset = Dataset.from_dict({
                "prompt": new_prompts,
                "chosen": new_chosens,
                "rejected": new_rejecteds,
                "text": new_texts,
                "event_types": new_event_types
            })

            # æ›´æ–° Trainer çš„æ•°æ®é›†
            self.trainer_ref.train_dataset = new_dataset

            # é‡å»º DataLoaderï¼ˆå…³é”®æ­¥éª¤ï¼‰
            if hasattr(self.trainer_ref, '_train_dataloader'):
                self.trainer_ref._train_dataloader = None
            if hasattr(self.trainer_ref, 'accelerator') and self.trainer_ref.accelerator is not None:
                # å¯¹äºä½¿ç”¨ accelerator çš„æƒ…å†µï¼Œéœ€è¦é‡æ–°å‡†å¤‡æ•°æ®é›†
                pass  # accelerator ä¼šåœ¨ä¸‹æ¬¡è¿­ä»£æ—¶è‡ªåŠ¨å¤„ç†
            if not self._refresh_warning_emitted:
                logger.warning(
                    "âš ï¸ åŠ¨æ€åˆ·æ–°ä¾èµ– trainer.train_dataset/_train_dataloader çš„å†…éƒ¨è¡Œä¸ºï¼›"
                    "å‡çº§ transformers/trl åè¯·åšå›å½’éªŒè¯ã€‚"
                )
                self._refresh_warning_emitted = True

            # ã€ä¿®å¤ã€‘é‡æ–°ç”Ÿæˆåå†è·å–ç»Ÿè®¡ï¼Œç¡®ä¿åæ˜ å½“å‰ Epoch çš„ç­–ç•¥åˆ†å¸ƒ
            new_stats = self.lans_scheduler.get_statistics()
            refresh_elapsed = time.perf_counter() - refresh_start_ts
            logger.info(
                f"âœ… Epoch {current_epoch}: å·²é‡æ–°ç”Ÿæˆ {len(new_dataset)} æ¡è´Ÿæ ·æœ¬ "
                f"(è€—æ—¶ {refresh_elapsed:.1f}s, æœ¬è½®ç­–ç•¥åˆ†å¸ƒ: {new_stats['strategy_distribution']})"
            )
    
    def on_step_end(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """æ¯ä¸ª Step ç»“æŸæ—¶æ›´æ–° LANS èƒ½åŠ›å€¼"""
        # [ä¿®å¤ T6] æ›´æ–° LANS é‡‡æ ·å™¨çš„è®­ç»ƒè¿›åº¦
        if self.lans_sampler is not None:
            total_steps = state.max_steps if state.max_steps > 0 else 1
            self.lans_sampler.set_training_progress(state.global_step, total_steps)

        if state.log_history:
            latest_log = state.log_history[-1]
            loss = latest_log.get("loss") or latest_log.get("train/loss") or latest_log.get("train_loss")
            
            if loss is not None:
                self._global_step += 1
                new_competence = self.lans_scheduler.update_competence(float(loss))
                
                if self._writer is not None:
                    stats = self.lans_scheduler.get_statistics()
                    self._writer.add_scalar("train/loss", float(loss), self._global_step)
                    self._writer.add_scalar("lans/competence", stats["competence"], self._global_step)
                    self._writer.add_scalar("lans/threshold", stats["threshold"], self._global_step)
                    self._writer.add_scalar("lans/recent_avg_loss", stats["recent_avg_loss"], self._global_step)
                    dist = stats["strategy_distribution"]
                    self._writer.add_scalar("lans/strategy_easy", dist.get("EASY", 0.0), self._global_step)
                    self._writer.add_scalar("lans/strategy_medium", dist.get("MEDIUM", 0.0), self._global_step)
                    self._writer.add_scalar("lans/strategy_hard", dist.get("HARD", 0.0), self._global_step)
                    if self.trainer_ref is not None and hasattr(self.trainer_ref, "get_aux_metrics_snapshot"):
                        aux_metrics = self.trainer_ref.get_aux_metrics_snapshot()
                        if aux_metrics:
                            self._writer.add_scalar(
                                "train/pref_loss_weighted",
                                float(aux_metrics.get("pref_loss_weighted", 0.0)),
                                self._global_step,
                            )
                            self._writer.add_scalar(
                                "train/rpo_weight",
                                float(aux_metrics.get("rpo_weight", 0.0)),
                                self._global_step,
                            )
                            self._writer.add_scalar(
                                "train/rpo_sft_loss",
                                float(aux_metrics.get("rpo_sft_loss", 0.0)),
                                self._global_step,
                            )
                            self._writer.add_scalar(
                                "train/loss_combined",
                                float(aux_metrics.get("combined_loss", 0.0)),
                                self._global_step,
                            )

                if self._global_step % 50 == 0:
                    stats = self.lans_scheduler.get_statistics()
                    aux_info = ""
                    if self.trainer_ref is not None and hasattr(self.trainer_ref, "get_aux_metrics_snapshot"):
                        aux_metrics = self.trainer_ref.get_aux_metrics_snapshot()
                        if aux_metrics:
                            aux_info = (
                                f", RPO_w={aux_metrics.get('rpo_weight', 0.0):.4f}, "
                                f"SFT={aux_metrics.get('rpo_sft_loss', 0.0):.4f}"
                            )
                    logger.debug(
                        f"LANS [Step {self._global_step}]: "
                        f"Loss={loss:.4f}, C={new_competence:.4f}, "
                        f"ç­–ç•¥åˆ†å¸ƒ={stats['strategy_distribution']}{aux_info}"
                    )
    
    def on_train_end(
        self, 
        args, 
        state: TrainerState, 
        control: TrainerControl, 
        **kwargs
    ):
        """è®­ç»ƒç»“æŸæ—¶ä¿å­˜å†å²"""
        logger.info("ğŸ”„ on_train_end å›è°ƒå¼€å§‹...")
        
        # TensorBoard writer å…³é—­ï¼ˆæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼‰
        if self._writer is not None:
            try:
                logger.info("  ğŸ“Š æ­£åœ¨å…³é—­ TensorBoard writer...")
                self._writer.flush()
                self._writer.close()
                self._writer = None  # é˜²æ­¢é‡å¤å…³é—­
                logger.info("  âœ… TensorBoard writer å·²å…³é—­")
            except Exception as e:
                logger.warning(f"  âš ï¸ TensorBoard writer å…³é—­å¤±è´¥: {e}")

        # ä¿å­˜ LANS å†å²
        if self.lans_scheduler:
            try:
                logger.info("  ğŸ“ æ­£åœ¨å¯¼å‡º LANS å†å²...")
                history = self.lans_scheduler.export_history()
                output_path = os.path.join(args.output_dir, "lans_history.json")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(history, f, ensure_ascii=False, indent=2)
                logger.info(f"  âœ… LANS è®­ç»ƒå†å²å·²ä¿å­˜: {output_path}")
            except Exception as e:
                logger.warning(f"  âŒ ä¿å­˜ LANS å†å²å¤±è´¥: {e}")
        
        # å¯¼å‡º LANS ç”Ÿæˆçš„è´Ÿæ ·æœ¬å’Œ SCV è¿‡æ»¤æ ·æœ¬
        if self.lans_sampler:
            try:
                logger.info("  ğŸ“¦ æ­£åœ¨å¯¼å‡º LANS è´Ÿæ ·æœ¬...")
                export_result = self.lans_sampler.export_samples()
                if export_result:
                    for key, path in export_result.items():
                        logger.info(f"     âœ… {key}: {path}")
                
                # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
                stats = self.lans_sampler.get_statistics()
                logger.info(f"  ğŸ“Š LANS é‡‡æ ·ç»Ÿè®¡:")
                logger.info(f"     ç”Ÿæˆæ€»æ•°: {stats['total_generated']}")
                logger.info(f"     SCV è¿‡æ»¤: {stats['scv_filtered_count']} ({stats['scv_filter_rate']:.2%})")
                if "scv_cache_hit_rate" in stats:
                    logger.info(
                        f"     SCV ç¼“å­˜: hits={stats.get('scv_cache_hits', 0)}, "
                        f"misses={stats.get('scv_cache_misses', 0)}, "
                        f"hit_rate={stats.get('scv_cache_hit_rate', 0.0):.2%}"
                    )
            except Exception as e:
                logger.warning(f"  âŒ å¯¼å‡º LANS æ ·æœ¬å¤±è´¥: {e}")
        
        logger.info("ğŸ”„ on_train_end å›è°ƒå®Œæˆ")


# ============================================================================
# LANS åœ¨çº¿è´Ÿé‡‡æ ·ç”Ÿæˆå™¨
# ============================================================================

class LANSNegativeSampler:
    """
    LANS è´Ÿæ ·æœ¬ç”Ÿæˆå™¨
    æ”¯æŒè´Ÿæ ·æœ¬å¯¼å‡ºå’Œ SCV è¿‡æ»¤æ ·æœ¬è®°å½•
    """
    
    def __init__(
        self,
        ds_cns: DSCNSampler,
        scv: Optional[SemanticConsistencyVerifier] = None,
        export_dir: Optional[str] = None,  # å¯¼å‡ºç›®å½•
        scv_cache_enabled: bool = True,
        scv_cache_max_entries: int = 50000,
        scv_max_retries: int = 1,
    ):
        self.ds_cns = ds_cns
        self.scv = scv
        self.epoch = 0
        self.export_dir = export_dir
        self.scv_cache_enabled = bool(scv_cache_enabled)
        self.scv_cache_max_entries = max(1000, int(scv_cache_max_entries))
        self.scv_max_retries = max(0, int(scv_max_retries))
        
        # è®°å½•ç”Ÿæˆçš„è´Ÿæ ·æœ¬å’Œ SCV è¿‡æ»¤æ ·æœ¬
        self._generated_samples: List[Dict] = []
        self._scv_filtered_samples: List[Dict] = []
        self._sample_counter = 0
        # [ä¿®å¤ T6] æ·»åŠ è®­ç»ƒè¿›åº¦è¿½è¸ª
        self._current_step = 0
        self._total_steps = 1
        self._scv_cache: "OrderedDict[str, bool]" = OrderedDict()
        self._scv_cache_hits = 0
        self._scv_cache_misses = 0

    @staticmethod
    def _stable_hash(text: str) -> str:
        return hashlib.sha1(text.encode("utf-8", errors="ignore")).hexdigest()

    def _get_scv_cache_key(self, text: str, neg_json: str) -> str:
        return self._stable_hash(f"{text}\n<SEP>\n{neg_json}")

    def _is_false_negative(self, text: str, neg_json: str) -> bool:
        if self.scv is None:
            return False

        if not self.scv_cache_enabled:
            return bool(self.scv.is_false_negative(text, neg_json))

        key = self._get_scv_cache_key(text, neg_json)
        cached = self._scv_cache.get(key)
        if cached is not None:
            self._scv_cache_hits += 1
            self._scv_cache.move_to_end(key)
            return bool(cached)

        self._scv_cache_misses += 1
        result = bool(self.scv.is_false_negative(text, neg_json))
        self._scv_cache[key] = result
        self._scv_cache.move_to_end(key)

        if len(self._scv_cache) > self.scv_cache_max_entries:
            self._scv_cache.popitem(last=False)
        return result

    def set_epoch(self, epoch: int):
        self.epoch = epoch

    def set_training_progress(self, current_step: int, total_steps: int):
        """è®¾ç½®å½“å‰è®­ç»ƒè¿›åº¦ï¼Œç”¨äº LANS è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—"""
        self._current_step = current_step
        self._total_steps = max(1, total_steps)

    def generate_rejected(self, example: Dict) -> Dict:
        chosen = example["chosen"]
        text = example.get("text", "")
        event_types = example.get("event_types") or []
        prompt = example.get("prompt", "")

        if self.ds_cns._use_lans:
            strategy = self.ds_cns.get_negative_strategy_adaptive()
        else:
            strategy = "MEDIUM"

        strategy_fallback = {"HARD": "MEDIUM", "MEDIUM": "EASY", "EASY": "EASY"}
        scv_filtered = False
        scv_retry_count = 0
        neg_json = ""
        current_strategy = strategy

        for attempt in range(self.scv_max_retries + 1):
            # [ä¿®å¤ T6] ä¼ é€’å®é™…çš„è®­ç»ƒæ­¥æ•°ï¼Œä½¿ LANS é˜ˆå€¼ç”Ÿæ•ˆ
            neg_json = self.ds_cns.generate_negative_json(
                chosen, current_strategy, self._current_step, self._total_steps
            )

            if self.scv is None:
                break

            try:
                is_false_neg = self._is_false_negative(text, neg_json)
            except Exception:
                is_false_neg = False

            if not is_false_neg:
                break

            scv_filtered = True
            scv_retry_count += 1
            self._scv_filtered_samples.append({
                "sample_id": self._sample_counter,
                "text_preview": text[:200] if text else "",
                "chosen": chosen[:500] if chosen else "",
                "filtered_rejected": neg_json[:500] if neg_json else "",
                "strategy": current_strategy,
                "attempt": attempt,
                "reason": "SCV detected false negative"
            })

            if attempt >= self.scv_max_retries:
                break

            current_strategy = strategy_fallback.get(current_strategy, "EASY")

        strategy = current_strategy

        rejected_cot = ChinesePromptBuilder.build_incorrect_cot_response(
            neg_json, strategy, original_types=event_types
        )
        
        # è®°å½•ç”Ÿæˆçš„è´Ÿæ ·æœ¬
        self._generated_samples.append({
            "sample_id": self._sample_counter,
            "prompt_preview": prompt[:200] if prompt else "",
            "chosen_preview": chosen[:300] if chosen else "",
            "rejected_preview": rejected_cot[:300] if rejected_cot else "",
            "strategy": strategy,
            "scv_filtered": scv_filtered,
            "scv_retry_count": scv_retry_count,
            "epoch": self.epoch
        })
        self._sample_counter += 1
        
        return {
            "prompt": example["prompt"],
            "chosen": example["chosen"],
            "rejected": rejected_cot
        }
    
    def export_samples(self, output_dir: Optional[str] = None) -> Dict[str, str]:
        """
        å¯¼å‡ºç”Ÿæˆçš„è´Ÿæ ·æœ¬å’Œ SCV è¿‡æ»¤æ ·æœ¬
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„ export_dir
        
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        export_path = output_dir or self.export_dir
        if not export_path:
            logger.warning("æœªæŒ‡å®šå¯¼å‡ºç›®å½•ï¼Œè·³è¿‡æ ·æœ¬å¯¼å‡º")
            return {}
        
        os.makedirs(export_path, exist_ok=True)
        result = {}
        
        # å¯¼å‡º LANS ç”Ÿæˆçš„è´Ÿæ ·æœ¬
        if self._generated_samples:
            neg_samples_file = os.path.join(export_path, "lans_generated_samples.jsonl")
            with open(neg_samples_file, 'w', encoding='utf-8') as f:
                for sample in self._generated_samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + "\n")
            result["generated_samples"] = neg_samples_file
            logger.info(f"ğŸ“¦ å¯¼å‡º LANS è´Ÿæ ·æœ¬: {len(self._generated_samples)} æ¡ -> {neg_samples_file}")
        
        # å¯¼å‡º SCV è¿‡æ»¤æ ·æœ¬
        if self._scv_filtered_samples:
            scv_filtered_file = os.path.join(export_path, "scv_filtered_samples.jsonl")
            with open(scv_filtered_file, 'w', encoding='utf-8') as f:
                for sample in self._scv_filtered_samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + "\n")
            result["scv_filtered"] = scv_filtered_file
            logger.info(f"ğŸ” å¯¼å‡º SCV è¿‡æ»¤æ ·æœ¬: {len(self._scv_filtered_samples)} æ¡ -> {scv_filtered_file}")
        
        # å¯¼å‡ºç»Ÿè®¡æ‘˜è¦
        summary = {
            "total_generated": len(self._generated_samples),
            "scv_filtered_count": len(self._scv_filtered_samples),
            "scv_filter_rate": len(self._scv_filtered_samples) / max(1, len(self._generated_samples)),
            "scv_cache": {
                "enabled": self.scv_cache_enabled,
                "max_entries": self.scv_cache_max_entries,
                "size": len(self._scv_cache),
                "hits": self._scv_cache_hits,
                "misses": self._scv_cache_misses,
                "hit_rate": self._scv_cache_hits / max(1, self._scv_cache_hits + self._scv_cache_misses),
            },
            "strategy_distribution": {}
        }
        
        # ç»Ÿè®¡ç­–ç•¥åˆ†å¸ƒ
        for sample in self._generated_samples:
            strategy = sample.get("strategy", "UNKNOWN")
            summary["strategy_distribution"][strategy] = summary["strategy_distribution"].get(strategy, 0) + 1
        
        summary_file = os.path.join(export_path, "lans_sampling_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        result["summary"] = summary_file
        
        return result
    
    def get_statistics(self) -> Dict:
        """è·å–é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_generated": len(self._generated_samples),
            "scv_filtered_count": len(self._scv_filtered_samples),
            "scv_filter_rate": len(self._scv_filtered_samples) / max(1, len(self._generated_samples)),
            "scv_cache_hits": self._scv_cache_hits,
            "scv_cache_misses": self._scv_cache_misses,
            "scv_cache_hit_rate": self._scv_cache_hits / max(1, self._scv_cache_hits + self._scv_cache_misses),
        }


class LANSIterableDataset(IterableDataset):
    """
    åœ¨çº¿ LANS è´Ÿæ ·æœ¬æ•°æ®é›†ï¼ˆçœŸæ­£çš„ Online Adaptive é‡‡æ ·ï¼‰
    æ¯æ¬¡è¿­ä»£éƒ½ä¼šè¯»å–æœ€æ–°çš„ competence å¹¶åŠ¨æ€ç”Ÿæˆ rejectedã€‚

    âš ï¸ è­¦å‘Šï¼šæ­¤ç±»ä¸æ”¯æŒ num_workers > 0ï¼Œå› ä¸º Worker è¿›ç¨‹æ— æ³•åŒæ­¥ä¸»è¿›ç¨‹çš„èƒ½åŠ›å€¼æ›´æ–°ã€‚
    è¯·ç¡®ä¿ DPOConfig ä¸­è®¾ç½® dataloader_num_workers=0ã€‚
    """

    def __init__(self, base_samples: List[Dict], lans_sampler: LANSNegativeSampler, seed: int = 3407):
        import warnings
        warnings.warn(
            "LANSIterableDataset ä»…æ”¯æŒ dataloader_num_workers=0ã€‚"
            "å¤šè¿›ç¨‹æ¨¡å¼ä¸‹ LANS èƒ½åŠ›å€¼æ— æ³•åŒæ­¥ï¼ŒåŠ¨æ€é‡‡æ ·å°†å¤±æ•ˆã€‚",
            UserWarning
        )
        self.base_samples = base_samples
        self.lans_sampler = lans_sampler
        self.seed = seed
        self.epoch = 0

    def set_epoch(self, epoch: int):
        self.epoch = epoch

    def __iter__(self):
        rng = random.Random(self.seed + self.epoch)
        indices = list(range(len(self.base_samples)))
        rng.shuffle(indices)

        for idx in indices:
            item = self.base_samples[idx]
            yield self.lans_sampler.generate_rejected(item)


class LANSDataCollator:
    """
    [Legacy] LANS åŠ¨æ€è´Ÿé‡‡æ · Collatorï¼ˆé»˜è®¤è®­ç»ƒè·¯å¾„æœªå¯ç”¨ï¼‰

    å†å²å®ç°ï¼šåœ¨ DataLoader æ‰¹æ¬¡ç”Ÿæˆæ—¶åŠ¨æ€è°ƒç”¨ LANS é‡‡æ ·å™¨ç”Ÿæˆè´Ÿæ ·æœ¬ã€‚
    å½“å‰é»˜è®¤ä¸»è·¯å¾„é‡‡ç”¨â€œåˆå§‹å…¨é‡ + æ¯ Epoch åˆ·æ–°â€çš„è®­ç»ƒæœºåˆ¶ï¼Œä¸å¯ç”¨ per-batch collatorã€‚
    è¯¥ç±»ä»…ä¿ç•™ç”¨äºå¯¹ç…§å®éªŒä¸å‘åå…¼å®¹ã€‚
    """

    def __init__(self, tokenizer, lans_sampler: LANSNegativeSampler, max_length: int = 2048):
        self.tokenizer = tokenizer
        self.lans_sampler = lans_sampler
        self.max_length = max_length
        # ä½¿ç”¨ DPODataCollatorWithPadding å¤„ç†æœ€ç»ˆçš„ Padding
        self.base_collator = DPODataCollatorWithPadding(tokenizer, max_length=max_length)

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å¤„ç†ä¸€ä¸ª Batch çš„æ•°æ®ï¼š
        1. æå–åŸå§‹ä¿¡æ¯ (chosen, prompt, etc)
        2. è°ƒç”¨ lans_sampler ç”ŸæˆåŠ¨æ€ rejected
        3. Tokenize æ–°çš„ rejected
        4. æ„é€  labels (mask prompt)
        5. è°ƒç”¨ base_collator è¿›è¡Œ Padding
        """
        # é¢„å…ˆè®¡ç®— pad_token_id (å¦‚æœ base_collator éœ€è¦ï¼Œè™½ç„¶åé¢è°ƒç”¨äº†å®ƒ)

        for feature in features:
            # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ (DPOTrainer remove_unused_columns=False åº”ä¿ç•™è¿™äº›)
            if "prompt" not in feature or "chosen" not in feature:
                continue

            # 1. æ„é€ é‡‡æ ·æ‰€éœ€çš„æ ·æœ¬å­—å…¸
            sample = {
                "chosen": feature["chosen"],
                "text": feature.get("text", ""),
                "event_types": feature.get("event_types", []),
                "prompt": feature.get("prompt", "")
            }

            # 2. åŠ¨æ€ç”Ÿæˆ Negative (åŸºäºå½“å‰ LANS Competence)
            # æ³¨æ„ï¼šgenerate_rejected å†…éƒ¨ä¼šè¯»å– lans_scheduler çš„æœ€æ–°çŠ¶æ€
            result = self.lans_sampler.generate_rejected(sample)
            rejected_content = result["rejected"]

            # 3. Tokenize Prompt + Rejected
            # prompt å·²ç»åŒ…å« Chat Template æ ¼å¼
            prompt = feature["prompt"]
            full_rejected_text = prompt + rejected_content + self.tokenizer.eos_token

            tokenized_rejected = self.tokenizer(
                full_rejected_text,
                truncation=True,
                max_length=self.max_length,
                add_special_tokens=False  # Prompt å·²å«ç‰¹æ®Š token
            )

            rejected_input_ids = tokenized_rejected["input_ids"]
            rejected_attention_mask = tokenized_rejected["attention_mask"]

            # 4. æ„é€  Rejected Labels (Mask Prompt éƒ¨åˆ†)
            # ä¸ºäº†å‡†ç¡® Maskï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ Prompt çš„é•¿åº¦
            # Tokenize Prompt å•ç‹¬è·å–é•¿åº¦
            prompt_tokens = self.tokenizer(
                prompt,
                truncation=True,
                max_length=self.max_length,
                add_special_tokens=False
            )["input_ids"]
            prompt_len = len(prompt_tokens)

            rejected_labels = list(rejected_input_ids)
            # å°† Prompt éƒ¨åˆ†è®¾ä¸º -100 (Ignore Index)
            for i in range(min(prompt_len, len(rejected_labels))):
                rejected_labels[i] = -100

            # 5. æ›´æ–° feature (è¦†ç›– DPOTrainer é¢„å¤„ç†çš„ dummy rejected)
            feature["rejected_input_ids"] = rejected_input_ids
            feature["rejected_attention_mask"] = rejected_attention_mask
            feature["rejected_labels"] = rejected_labels

            # chosen_input_ids ç­‰å­—æ®µä¿æŒä¸å˜ (ç”± DPOTrainer é¢„å¤„ç†å¥½)

        # 6. äº¤ç»™ Base Collator è¿›è¡Œ Padding å’Œ Tensor è½¬æ¢
        return self.base_collator(features)


class UnslothDPOTrainerWrapper:
    def __init__(self, config: dict, data_samples: list):
        init_start_ts = time.perf_counter()
        self.config = config
        self.samples = data_samples
        self.runtime_stats: Dict[str, Any] = {
            "phase_timings_seconds": {},
        }
        
        ds_cns_cfg = config['algorithms']['ds_cns']
        schema_path = ds_cns_cfg['taxonomy_path']
        static_c0 = ds_cns_cfg.get('static_mode_c0', 0.1)
        use_ontology_distance = ds_cns_cfg.get('use_ontology_distance', True)  # ã€æ¶ˆèå®éªŒ A6ã€‘
        ds_cns_start_ts = time.perf_counter()
        self.ds_cns = DSCNSampler(
            schema_path, 
            c0=static_c0,
            use_ontology_distance=use_ontology_distance
        )
        self.runtime_stats["phase_timings_seconds"]["ds_cns_init"] = round(
            time.perf_counter() - ds_cns_start_ts, 4
        )
        
        self.scv_cfg = config.get('algorithms', {}).get('scv', {})
        self.scv = None
        if config['algorithms']['scv']['enabled']:
            scv_start_ts = time.perf_counter()
            self.scv = SemanticConsistencyVerifier(
                self.scv_cfg['nli_model'],
                self.scv_cfg['nli_threshold'],
                progress_log_interval=self.scv_cfg.get('progress_log_interval', 200),
                progress_log_seconds=self.scv_cfg.get('progress_log_seconds', 30),
            )
            self.runtime_stats["phase_timings_seconds"]["scv_init"] = round(
                time.perf_counter() - scv_start_ts, 4
            )
        else:
            self.runtime_stats["phase_timings_seconds"]["scv_init"] = 0.0
        
        self.lans_scheduler = None
        self.lans_callback = None
        self.lans_sampler = None
        self.runtime_stats["phase_timings_seconds"]["trainer_init"] = round(
            time.perf_counter() - init_start_ts, 4
        )

    def load_model(self) -> None:
        load_start_ts = time.perf_counter()
        m_cfg = self.config['model']
        l_cfg = self.config['lora']
        model_name_or_path = m_cfg['base_model']
        
        if m_cfg.get('source', 'huggingface') == 'modelscope':
            try:
                from modelscope import snapshot_download
                model_name_or_path = snapshot_download(model_name_or_path, cache_dir='./models')
            except Exception:
                pass
        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name = model_name_or_path,
            max_seq_length = m_cfg['max_seq_length'],
            dtype = None,
            load_in_4bit = m_cfg['load_in_4bit'],
        )
        self.model.gradient_checkpointing_enable()
        
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r = l_cfg['r'],
            target_modules = l_cfg['target_modules'],
            lora_alpha = l_cfg['lora_alpha'],
            lora_dropout = l_cfg['lora_dropout'],
            bias = l_cfg['bias'],
            use_gradient_checkpointing = "unsloth",
            random_state = 3407,
        )
        self.tokenizer.padding_side = "left"
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        # [ä¿®å¤] æ˜¾å¼æ£€æŸ¥ EOS tokenï¼ˆQwen3 é€šå¸¸ä½¿ç”¨ <|im_end|>ï¼‰
        expected_eos = "<|im_end|>"
        if self.tokenizer.eos_token is None or self.tokenizer.eos_token == "":
            self.tokenizer.eos_token = expected_eos
        elif self.tokenizer.eos_token != expected_eos:
            logger.warning(
                f"EOS token ä¸º {self.tokenizer.eos_token}ï¼ŒæœŸæœ› {expected_eos}ã€‚å°†ä¿ç•™å½“å‰è®¾ç½®ã€‚"
            )
        logger.info(
            f"EOS Token: {self.tokenizer.eos_token} | EOS Token ID: {self.tokenizer.eos_token_id}"
        )
        load_elapsed = time.perf_counter() - load_start_ts
        self.runtime_stats["phase_timings_seconds"]["model_load"] = round(load_elapsed, 4)
        logger.info(f"â±ï¸ æ¨¡å‹åŠ è½½é˜¶æ®µè€—æ—¶: {load_elapsed:.1f}s")

    def _apply_chat_template(self, raw_text: str) -> str:
        """[å…³é”®] åº”ç”¨ Chat Templateï¼Œç»Ÿä¸€è®­ç»ƒä¸è¯„ä¼°çš„ prompt æ„å»º"""
        return build_inference_prompt(
            text=raw_text,
            tokenizer=self.tokenizer,
            use_oneshot=False
        )

    def prepare_dpo_dataset(self, use_lans: bool = True) -> Any:
        seed = self.config['project']['seed']
        random.seed(seed)
        np.random.seed(seed)
        
        verified_samples = []
        for s in self.samples:
            if all(t in self.ds_cns.graph for t in (s.event_types or [])):
                verified_samples.append(s)
        self.samples = verified_samples
        total_steps = len(self.samples)
        
        if use_lans:
            logger.info("ğŸš€ å¯ç”¨ LANS (åœ¨çº¿æ¨¡å¼)")
            lans_cfg = self.config['algorithms'].get('lans', {})

            # ã€ä¿®å¤ã€‘è¯»å–å¤šç²’åº¦æƒé‡é…ç½®
            granularity_weights = lans_cfg.get('granularity_weights', None)
            loss_baseline = lans_cfg.get("loss_baseline")
            if loss_baseline is None and "lans_alpha" in lans_cfg:
                # å‘åå…¼å®¹æ—§é…ç½®é”®ï¼Œé¿å…å†å²å®éªŒé…ç½®ç›´æ¥å¤±æ•ˆ
                loss_baseline = lans_cfg.get("lans_alpha")
                logger.warning(
                    "é…ç½®é”® algorithms.lans.lans_alpha å·²å¼ƒç”¨ï¼Œè¯·æ”¹ç”¨ algorithms.lans.loss_baselineã€‚"
                )
            if loss_baseline is None:
                loss_baseline = 0.5

            self.lans_scheduler = self.ds_cns.enable_lans(
                ema_decay=lans_cfg.get('ema_decay', 0.95),
                loss_baseline=loss_baseline,
                warmup_steps=lans_cfg.get('warmup_steps', 100),
                competence_floor=lans_cfg.get('competence_floor', 0.05),
                competence_ceiling=lans_cfg.get('competence_ceiling', 0.95),
                warmup_target=lans_cfg.get('warmup_target', 0.25),
                use_ema=lans_cfg.get('use_ema', True),
                cga_beta=lans_cfg.get('cga_beta', 0.1),
                use_cga=lans_cfg.get('use_cga', True),
                granularity_weights=granularity_weights,
                easy_ratio=lans_cfg.get('strategies', {}).get('easy_ratio', 0.7),
                hard_ratio=lans_cfg.get('strategies', {}).get('hard_ratio', 0.4),
                hard_floor_prob=lans_cfg.get('strategies', {}).get('hard_floor_prob', 0.0),
                hard_floor_after_warmup=lans_cfg.get('strategies', {}).get('hard_floor_after_warmup'),
                medium_floor_prob=lans_cfg.get('strategies', {}).get('medium_floor_prob', 0.0),
            )

            # ä¼ é€’å¯¼å‡ºç›®å½•åˆ° LANSNegativeSampler
            export_dir = self.config['project'].get('debug_data_dir')
            self.lans_sampler = LANSNegativeSampler(
                ds_cns=self.ds_cns,
                scv=self.scv,
                export_dir=export_dir,
                scv_cache_enabled=self.scv_cfg.get("cache_enabled", True),
                scv_cache_max_entries=self.scv_cfg.get("cache_max_entries", 50000),
                scv_max_retries=self.scv_cfg.get("max_retries", 1),
            )

            samples_data = []
            for sample in self.samples:
                formatted_prompt = self._apply_chat_template(sample.text) # ä¿®å¤ Loss
                samples_data.append({
                    "prompt": formatted_prompt,
                    "chosen": sample.chosen,
                    "rejected": "",  # ã€å…³é”®ä¿®æ”¹ã€‘æä¾›ç©º Rejectedï¼Œç”± LANSDataCollator åŠ¨æ€ç”Ÿæˆ
                    "text": sample.text,
                    "event_types": sample.event_types or []
                })
            return samples_data

        else:
            logger.info("ğŸ“¦ ä½¿ç”¨é™æ€è¯¾ç¨‹å­¦ä¹ æ¨¡å¼")
            cache_dir = os.path.join(self.config['project']['dataset_cache_dir'], "dpo_dataset_cache")
            
            # ä¼˜å…ˆä»ç¼“å­˜åŠ è½½
            if os.path.exists(cache_dir):
                logger.info(f"   ğŸ“‚ ä»ç¼“å­˜åŠ è½½æ•°æ®é›†: {cache_dir}")
                return Dataset.load_from_disk(cache_dir)
            
            logger.info("   ğŸ”§ é¦–æ¬¡è¿è¡Œï¼Œç”Ÿæˆé™æ€æ•°æ®é›†...")
            prompts, chosens, rejecteds = [], [], []
            static_samples_log = []  # è®°å½•é™æ€æ ·æœ¬ç”¨äºå¯¼å‡º
            
            for idx, sample in enumerate(self.samples):
                formatted_prompt = self._apply_chat_template(sample.text) # ä¿®å¤ Loss
                strategy = self.ds_cns.get_negative_strategy(idx, total_steps)
                neg_json = self.ds_cns.generate_negative_json(sample.chosen, strategy, idx, total_steps)
                rejected_cot = ChinesePromptBuilder.build_incorrect_cot_response(
                    neg_json, strategy, sample.event_types
                )
                prompts.append(formatted_prompt)
                chosens.append(sample.chosen)
                rejecteds.append(rejected_cot)
                
                # è®°å½•ç”¨äºå¯¼å‡º
                static_samples_log.append({
                    "sample_id": idx,
                    "strategy": strategy,
                    "prompt_preview": formatted_prompt[:200],
                    "chosen_preview": sample.chosen[:300],
                    "rejected_preview": rejected_cot[:300]
                })
            
            dataset = Dataset.from_dict({"prompt": prompts, "chosen": chosens, "rejected": rejecteds})
            
            # ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜
            os.makedirs(cache_dir, exist_ok=True)
            dataset.save_to_disk(cache_dir)
            logger.info(f"   ğŸ’¾ æ•°æ®é›†å·²ç¼“å­˜: {cache_dir}")
            
            # å¯¼å‡ºé™æ€æ ·æœ¬æ—¥å¿—
            debug_dir = self.config['project'].get('debug_data_dir')
            if debug_dir:
                os.makedirs(debug_dir, exist_ok=True)
                static_log_file = os.path.join(debug_dir, "static_dpo_samples.jsonl")
                with open(static_log_file, 'w', encoding='utf-8') as f:
                    for sample in static_samples_log:
                        f.write(json.dumps(sample, ensure_ascii=False) + "\n")
                logger.info(f"   ğŸ“¦ é™æ€æ ·æœ¬æ—¥å¿—å·²å¯¼å‡º: {static_log_file}")
            
            return dataset

    def train(self, use_lans: bool = True) -> None:
        transformers.set_seed(self.config['project']['seed'])
        
        base_dataset = self.prepare_dpo_dataset(use_lans=use_lans)
        t_cfg = self.config['training']
        
        # ä»æ¨¡å‹é…ç½®è·å– max_seq_length
        m_cfg = self.config['model']
        max_seq_len = m_cfg.get('max_seq_length', 4096)
        
        use_online_lans = bool(use_lans and self.lans_scheduler is not None)
        fast_io = bool(t_cfg.get('fast_io', False))
        dataloader_num_workers = int(t_cfg.get('dataloader_num_workers', 0))
        dataloader_pin_memory = bool(t_cfg.get('dataloader_pin_memory', False))
        gradient_checkpointing = bool(t_cfg.get('gradient_checkpointing', True))

        if fast_io and not use_online_lans and dataloader_num_workers <= 0:
            dataloader_num_workers = 2
        if fast_io:
            dataloader_pin_memory = True
        if use_online_lans and dataloader_num_workers != 0:
            logger.warning(
                "LANS åŠ¨æ€é‡‡æ ·æ¨¡å¼ä¸‹å¼ºåˆ¶ dataloader_num_workers=0ï¼Œé¿å…å¤šè¿›ç¨‹çŠ¶æ€ä¸åŒæ­¥ã€‚"
            )
            dataloader_num_workers = 0

        dpo_config = DPOConfig(
            output_dir = self.config['project']['output_dir'],
            logging_dir = self.config['project']['logging_dir'],
            beta = t_cfg['beta'],
            loss_type = t_cfg['loss_type'],
            per_device_train_batch_size = t_cfg['per_device_train_batch_size'],
            gradient_accumulation_steps = t_cfg['gradient_accumulation_steps'],
            learning_rate = t_cfg['learning_rate'],
            lr_scheduler_type = t_cfg.get('lr_scheduler_type', 'cosine'),
            warmup_steps = t_cfg.get('warmup_steps', 0),
            num_train_epochs = t_cfg['num_train_epochs'],
            max_steps = t_cfg.get('max_steps', -1),
            logging_steps = t_cfg['logging_steps'],
            bf16 = t_cfg['bf16'],
            fp16 = False,  # ã€ä¿®å¤ã€‘bf16 å’Œ fp16 äº’æ–¥ï¼Œæ˜ç¡®ç¦ç”¨ fp16
            optim = t_cfg['optim'],
            weight_decay = t_cfg.get('weight_decay', 0.0),
            max_grad_norm = t_cfg.get('max_grad_norm', 1.0),
            # ã€å…³é”®ä¿®å¤ã€‘è°ƒæ•´é•¿åº¦å‚æ•°ï¼Œä¸æ¨¡å‹é…ç½®ä¸€è‡´
            max_prompt_length = min(2048, max_seq_len // 2),  # prompt å ä¸€åŠ
            max_length = max_seq_len,  # æ€»é•¿åº¦ä¸æ¨¡å‹ä¸€è‡´
            max_completion_length = max_seq_len // 2,  # é™åˆ¶å“åº”é•¿åº¦
            gradient_checkpointing = gradient_checkpointing,
            report_to = ["tensorboard"],
            remove_unused_columns = False,
            # ã€ä¿®å¤ã€‘æ­£ç¡®å¤„ç†ä¿å­˜ç­–ç•¥
            save_strategy = t_cfg.get('save_strategy', 'no'),  # é»˜è®¤ä¸è‡ªåŠ¨ä¿å­˜
            save_steps = t_cfg.get('save_steps', 500) if t_cfg.get('save_strategy', 'no') != 'no' else 500,
            save_total_limit = t_cfg.get('save_total_limit', 2),
            # ã€ä¿®å¤ã€‘ç°åœ¨ä½¿ç”¨æ™®é€š Datasetï¼Œç¦ç”¨ precompute ä»¥é˜²æ­¢ OOM
            precompute_ref_log_probs = False,
            # ã€ä¿®å¤ã€‘æ˜¾å¼è®¾ç½® dataloader å‚æ•°
            dataloader_num_workers = dataloader_num_workers,
            dataloader_pin_memory = dataloader_pin_memory,
        )
        
        save_info = f"Save={dpo_config.save_strategy}" if dpo_config.save_strategy != 'no' else "Save=manual"
        print(
            f"\nğŸš€ Training Config: Steps={dpo_config.max_steps}, {save_info}, "
            f"GC={dpo_config.gradient_checkpointing}, workers={dpo_config.dataloader_num_workers}, "
            f"pin_memory={dpo_config.dataloader_pin_memory}"
        )
        if dpo_config.gradient_checkpointing:
            logger.info("â„¹ï¸ gradient_checkpointing å·²å¯ç”¨ï¼šæ›´çœæ˜¾å­˜ï¼Œä½†è®­ç»ƒååé€šå¸¸ä¼šä¸‹é™ã€‚")

        # RPO mixed objective: loss = preference_loss + alpha * SFT(chosen)
        rpo_cfg = t_cfg.get("rpo", {})
        rpo_alpha = float(rpo_cfg.get("alpha", t_cfg.get("rpo_alpha", 0.0)))
        rpo_warmup_steps = int(rpo_cfg.get("warmup_steps", t_cfg.get("rpo_warmup_steps", 0)))
        aux_log_interval = int(rpo_cfg.get("log_interval", t_cfg.get("aux_log_interval", 50)))
        if rpo_alpha > 0:
            logger.info(
                f"ğŸ¯ è®­ç»ƒç›®æ ‡: Preference + RPO(SFT), alpha={rpo_alpha:.4f}, "
                f"warmup_steps={rpo_warmup_steps}"
            )
        
        callbacks = []
        # é»˜è®¤é‡‡ç”¨ Epoch çº§åˆ·æ–°ï¼Œä¸èµ° per-batch DataCollator åŠ¨æ€é‡‡æ ·
        data_collator = None
        lans_cfg = self.config.get('algorithms', {}).get('lans', {})

        if use_online_lans:
            logger.info("ğŸ”„ å¯ç”¨ LANS è‡ªé€‚åº”é‡‡æ ·æ¨¡å¼ (Epoch çº§åˆ«åŠ¨æ€åˆ·æ–°)")
            logger.info("â„¹ï¸ å½“å‰ä¸å¯ç”¨ LANSDataCollatorï¼Œè´Ÿæ ·æœ¬ç”±åˆå§‹åŒ–é˜¶æ®µä¸æ¯ Epoch åˆ·æ–°ç”Ÿæˆ")

            # ã€é‡è¦ã€‘å…ˆä½¿ç”¨åˆå§‹èƒ½åŠ›å€¼ç”Ÿæˆç¬¬ä¸€æ‰¹è´Ÿæ ·æœ¬
            logger.info("ğŸ“¦ ç”Ÿæˆåˆå§‹è´Ÿæ ·æœ¬ (Epoch 0)...")
            prompts, chosens, rejecteds = [], [], []
            texts, event_types_list = [], []

            total_samples = len(base_dataset)
            refresh_log_interval = max(1, int(lans_cfg.get("refresh_log_interval", 100)))
            refresh_log_seconds = max(5.0, float(lans_cfg.get("refresh_log_seconds", 30)))
            init_start_ts = time.perf_counter()
            last_log_ts = init_start_ts
            for idx, sample in enumerate(base_dataset):
                current = idx + 1
                now = time.perf_counter()
                should_log = (
                    current % refresh_log_interval == 0
                    or current == total_samples
                    or (now - last_log_ts) >= refresh_log_seconds
                )
                if should_log:
                    elapsed = max(now - init_start_ts, 1e-6)
                    speed = current / elapsed
                    eta = max(total_samples - current, 0) / max(speed, 1e-6)
                    logger.info(
                        f"   â³ åˆå§‹è´Ÿæ ·æœ¬è¿›åº¦: {current}/{total_samples} "
                        f"({current / max(total_samples, 1):.1%}), {speed:.2f} samples/s, ETA {eta:.1f}s"
                    )
                    last_log_ts = now

                result = self.lans_sampler.generate_rejected(sample)
                prompts.append(result["prompt"])
                chosens.append(result["chosen"])
                rejecteds.append(result["rejected"])
                texts.append(sample.get("text", ""))
                event_types_list.append(sample.get("event_types", []))

            dataset = Dataset.from_dict({
                "prompt": prompts,
                "chosen": chosens,
                "rejected": rejecteds,
                "text": texts,
                "event_types": event_types_list
            })

            init_elapsed = time.perf_counter() - init_start_ts
            init_speed = len(dataset) / max(init_elapsed, 1e-6)
            logger.info(
                f"âœ… ç”Ÿæˆ {len(dataset)} æ¡åˆå§‹è´Ÿæ ·æœ¬ "
                f"(è€—æ—¶ {init_elapsed:.1f}s, åå {init_speed:.2f} samples/s)"
            )
            self.runtime_stats["phase_timings_seconds"]["initial_negative_generation"] = round(
                init_elapsed, 4
            )
            if self.scv is not None:
                self.runtime_stats["scv_runtime"] = {
                    "calls": int(getattr(self.scv, "_calls", 0)),
                    "total_windows": int(getattr(self.scv, "_total_windows", 0)),
                    "total_time_seconds": round(float(getattr(self.scv, "_total_time_seconds", 0.0)), 4),
                }
                logger.info(
                    "ğŸ” SCV é˜¶æ®µæ±‡æ€»: "
                    f"calls={self.runtime_stats['scv_runtime']['calls']}, "
                    f"windows={self.runtime_stats['scv_runtime']['total_windows']}, "
                    f"time={self.runtime_stats['scv_runtime']['total_time_seconds']:.1f}s"
                )

            # å¯¼å‡ºåˆå§‹æ ·æœ¬ç”¨äºè°ƒè¯•
            if self.lans_sampler:
                self.lans_sampler.export_samples()

        else:
            dataset = base_dataset
            self.runtime_stats["phase_timings_seconds"]["initial_negative_generation"] = 0.0

        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ CGADPOTrainer æ›¿ä»£æ ‡å‡† DPOTrainerï¼Œå¯ç”¨å¯¹æ¯”æ¢¯åº¦æ”¾å¤§
        # [ä¿®å¤ T7] ä½¿ç”¨ processing_class æ›¿ä»£ tokenizerï¼ˆTRL >= 0.9.0ï¼‰
        trainer = CGADPOTrainer(
            model = self.model,
            ref_model = None,
            processing_class = self.tokenizer,
            train_dataset = dataset,
            data_collator = data_collator,
            args = dpo_config,
            callbacks = [],  # å…ˆä¼ ç©ºï¼Œåé¢æ·»åŠ 
            lans_scheduler = self.lans_scheduler if use_online_lans else None,
            rpo_alpha = rpo_alpha,
            rpo_warmup_steps = rpo_warmup_steps,
            aux_log_interval = aux_log_interval,
        )

        # ã€æ ¸å¿ƒã€‘åˆ›å»º LANS Callback å¹¶ä¼ é€’ trainer å¼•ç”¨
        if use_online_lans:
            self.lans_callback = LANSCallback(
                self.lans_scheduler,
                lans_sampler=self.lans_sampler,
                lans_dataset=None,
                logging_dir=dpo_config.logging_dir,
                trainer_ref=trainer,  # ã€æ–°å¢ã€‘ä¼ é€’ Trainer å¼•ç”¨
                base_samples=base_dataset,  # ã€æ–°å¢ã€‘ä¼ é€’åŸå§‹æ ·æœ¬
                tokenizer=self.tokenizer,  # ã€æ–°å¢ã€‘ä¼ é€’ Tokenizer
                refresh_start_epoch=lans_cfg.get("refresh_start_epoch", 1),
                refresh_log_interval=lans_cfg.get("refresh_log_interval", 100),
                refresh_log_seconds=lans_cfg.get("refresh_log_seconds", 30),
            )
            trainer.add_callback(self.lans_callback)
        
        logger.info(f"Starting Training...")
        train_loop_start_ts = time.perf_counter()
        try:
            trainer.train()
            logger.info("âœ… trainer.train() å®Œæˆ")
            self.runtime_stats["phase_timings_seconds"]["train_loop"] = round(
                time.perf_counter() - train_loop_start_ts, 4
            )

        except Exception as e:
            logger.error(f"Training interrupted: {e}")
            raise e
        
        # [ä¼˜åŒ–] æ˜¾å¼æ¸…ç†ç¼“å­˜ï¼Œé˜²æ­¢ä¿å­˜æ—¶ OOM æˆ–å¡é¡¿
        logger.info("ğŸ§¹ Training finished. Cleaning up memory before saving...")
        import gc
        gc.collect()
        
        # CUDA åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        torch.cuda.empty_cache()
        logger.info("  âœ… å†…å­˜æ¸…ç†å®Œæˆ")

        logger.info(f"ğŸ’¾ Saving model to {dpo_config.output_dir}...")
        save_start_ts = time.perf_counter()
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(dpo_config.output_dir, exist_ok=True)
            
            self.model.save_pretrained(dpo_config.output_dir)
            logger.info("  âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜")
            
            self.tokenizer.save_pretrained(dpo_config.output_dir)
            logger.info("  âœ… Tokenizer å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            raise e
            
        self.runtime_stats["phase_timings_seconds"]["save_artifacts"] = round(
            time.perf_counter() - save_start_ts, 4
        )
        timings = self.runtime_stats.get("phase_timings_seconds", {})
        logger.info(
            "â±ï¸ è®­ç»ƒé˜¶æ®µè€—æ—¶æ±‡æ€»(s): "
            + ", ".join([f"{k}={v}" for k, v in timings.items()])
        )
        logger.info("âœ… Model saved successfully.")

    def get_runtime_stats(self) -> Dict[str, Any]:
        return dict(self.runtime_stats)
