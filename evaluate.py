# evaluate.py
"""
OG-LANS å­¦æœ¯çº§è¯„ä¼°è„šæœ¬ (Academic Evaluation Framework)
é¢å‘ 2026 å¹´é«˜è´¨é‡è®ºæ–‡å‘è¡¨

å®ç°åŠŸèƒ½:
1. Strict/Relaxed ä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼ˆç¬¦åˆ ACL/EMNLP è§„èŒƒï¼‰
2. é²æ£’ JSON è§£æï¼ˆé›†æˆ RobustJSONParserï¼‰
3. å¤šç»´åº¦æŒ‡æ ‡ï¼ˆType F1, Role F1, Argument F1ï¼‰
4. è¯¦ç»†çš„é”™è¯¯åˆ†ææŠ¥å‘Š
5. å¹»è§‰æ£€æµ‹ç‡ (Hallucination Rate)
6. CoT å¿ å®åº¦ (CoT Faithfulness)
7. Schema ç¬¦åˆåº¦ (Schema Compliance)

è®ºæ–‡å‘è¡¨æ”¯æŒ:
- æä¾›å®Œæ•´çš„ LaTeX è¡¨æ ¼æ ¼å¼è¾“å‡º
- æ”¯æŒæ¶ˆèå®éªŒå¯¹æ¯”åˆ†æ
- ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•ï¼ˆBootstrapï¼‰
"""

import os
import json
import yaml
import argparse
import re
import random
import time
import copy
from tqdm import tqdm
from collections import defaultdict
from dataclasses import dataclass, field
from typing import List, Dict, Set, Tuple, Optional, Any

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from oglans.data.adapter import DuEEFinAdapter
from oglans.utils.json_parser import RobustJSONParser, parse_llm_output
from oglans.data.prompt_builder import ChinesePromptBuilder, build_inference_prompt
from oglans.utils.run_manifest import (
    build_run_manifest,
    collect_runtime_manifest,
    compute_file_sha256,
    save_json,
)
from oglans.utils.model_quantization import is_quantized_model


# ===========================
# 1. æ•°æ®ç»“æ„å®šä¹‰
# ===========================
@dataclass
class EvaluationResult:
    """å•æ ·æœ¬è¯„ä¼°ç»“æœ"""
    sample_id: str
    text_preview: str
    ground_truth: List[Dict]
    prediction: List[Dict]
    raw_response: str
    parse_success: bool
    parse_diagnostics: Dict = field(default_factory=dict)


@dataclass 
class MetricsReport:
    """è¯„ä¼°æŒ‡æ ‡æŠ¥å‘Š (2026 å­¦æœ¯è®ºæ–‡ç‰ˆ)"""
    # Strict æ¨¡å¼æŒ‡æ ‡
    strict_precision: float = 0.0
    strict_recall: float = 0.0
    strict_f1: float = 0.0
    
    # Relaxed æ¨¡å¼æŒ‡æ ‡
    relaxed_precision: float = 0.0
    relaxed_recall: float = 0.0
    relaxed_f1: float = 0.0
    
    # äº‹ä»¶ç±»å‹è¯†åˆ«æŒ‡æ ‡
    type_precision: float = 0.0
    type_recall: float = 0.0
    type_f1: float = 0.0
    
    # è§£æç»Ÿè®¡
    total_samples: int = 0
    parse_errors: int = 0
    parse_error_rate: float = 0.0
    
    # å¹»è§‰æ£€æµ‹æŒ‡æ ‡
    hallucination_rate: float = 0.0  # åŒ…å«å¹»è§‰çš„æ ·æœ¬æ¯”ä¾‹
    hallucination_entity_rate: float = 0.0  # å¹»è§‰å®ä½“å æ¯”
    
    # CoT å¿ å®åº¦æŒ‡æ ‡
    cot_faithfulness: float = 0.0  # CoT æ¨ç†ä¸ JSON è¾“å‡ºçš„ä¸€è‡´æ€§
    cot_type_consistency: float = 0.0  # äº‹ä»¶ç±»å‹ä¸€è‡´æ€§
    cot_argument_consistency: float = 0.0  # è®ºå…ƒä¸€è‡´æ€§
    
    # Schema ç¬¦åˆåº¦
    schema_compliance_rate: float = 0.0  # è¾“å‡ºç¬¦åˆ schema çš„æ¯”ä¾‹
    
    # è¯¦ç»†é”™è¯¯åˆ†æ
    error_breakdown: Dict = field(default_factory=dict)


# ===========================
# 2. æ ¸å¿ƒè¯„ä¼°å™¨ç±»
# ===========================
class AcademicEventEvaluator:
    """
    å­¦æœ¯çº§äº‹ä»¶æŠ½å–è¯„ä¼°å™¨
    
    æ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼:
    - Strict: (event_type, role, argument) å®Œå…¨åŒ¹é…
    - Relaxed: argument éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
    """
    
    def __init__(self, relaxed_match_threshold: float = 0.5):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            relaxed_match_threshold: Relaxed æ¨¡å¼çš„æœ€å°é‡å æ¯”ä¾‹
        """
        self.relaxed_threshold = relaxed_match_threshold
        self.json_parser = RobustJSONParser()
        
        # ç»Ÿè®¡æ•°æ®
        self.reset()
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡æ•°æ®"""
        self.stats = {
            # Strict æ¨¡å¼
            "strict_tp": 0,
            "strict_pred_total": 0,
            "strict_gold_total": 0,
            
            # Relaxed æ¨¡å¼
            "relaxed_tp": 0,
            "relaxed_pred_total": 0,
            "relaxed_gold_total": 0,
            
            # äº‹ä»¶ç±»å‹
            "type_tp": 0,
            "type_pred_total": 0,
            "type_gold_total": 0,
            
            # è§£æç»Ÿè®¡
            "total_samples": 0,
            "parse_errors": 0,
            
            # é”™è¯¯ç±»å‹åˆ†å¸ƒ
            "error_types": defaultdict(int),
            
            # å¹»è§‰æ£€æµ‹
            "hallucination_samples": 0,
            "total_entities": 0,
            "hallucinated_entities": 0,
            
            # CoT å¿ å®åº¦
            "cot_checked": 0,
            "cot_type_consistent": 0,
            "cot_argument_consistent": 0,
            "cot_fully_consistent": 0,
            
            # Schema ç¬¦åˆåº¦
            "schema_compliant": 0
        }
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """
        æ–‡æœ¬å½’ä¸€åŒ–ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            å½’ä¸€åŒ–åçš„æ–‡æœ¬
        """
        if text is None:
            return ""
        
        # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆargument å¯èƒ½æ˜¯æ•°å­—ç±»å‹ï¼‰
        if not isinstance(text, str):
            text = str(text)
        
        if not text:
            return ""
        
        # 1. ç§»é™¤ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', '', text)
        
        # 2. ç»Ÿä¸€å…¨è§’/åŠè§’
        text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')
        
        # 3. è½¬å°å†™ï¼ˆå¯¹äºè‹±æ–‡éƒ¨åˆ†ï¼‰
        text = text.lower()
        
        return text
    
    def extract_triplets_strict(self, events: List[Dict]) -> Set[Tuple[str, str, str]]:
        """
        æå– Strict æ¨¡å¼ä¸‰å…ƒç»„: (event_type, role, normalized_argument)
        
        Args:
            events: äº‹ä»¶åˆ—è¡¨
        
        Returns:
            ä¸‰å…ƒç»„é›†åˆ
        """
        triplets = set()
        
        if not isinstance(events, list):
            return triplets
        
        for event in events:
            if not isinstance(event, dict):
                continue
            
            event_type = event.get("event_type", "")
            if not event_type:
                continue
            
            arguments = event.get("arguments", [])
            if not isinstance(arguments, list):
                continue
            
            for arg in arguments:
                if not isinstance(arg, dict):
                    continue
                
                role = arg.get("role", "")
                argument = arg.get("argument", "")
                
                # ç¡®ä¿ argument æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if argument is not None and not isinstance(argument, str):
                    argument = str(argument)
                
                if role and argument:
                    norm_arg = self.normalize_text(argument)
                    if norm_arg:  # åªæœ‰éç©ºå€¼æ‰è®¡å…¥
                        triplets.add((event_type, role, norm_arg))
        
        return triplets
    
    def extract_triplets_relaxed(self, events: List[Dict]) -> List[Tuple[str, str, str]]:
        """
        æå– Relaxed æ¨¡å¼ä¸‰å…ƒç»„ï¼ˆä¿ç•™åŸå§‹ argument ç”¨äºéƒ¨åˆ†åŒ¹é…ï¼‰
        
        Returns:
            ä¸‰å…ƒç»„åˆ—è¡¨ï¼ˆéé›†åˆï¼Œå› ä¸ºéœ€è¦éå†æ¯”è¾ƒï¼‰
        """
        triplets = []
        
        if not isinstance(events, list):
            return triplets
        
        for event in events:
            if not isinstance(event, dict):
                continue
            
            event_type = event.get("event_type", "")
            if not event_type:
                continue
            
            arguments = event.get("arguments", [])
            if not isinstance(arguments, list):
                continue
            
            for arg in arguments:
                if not isinstance(arg, dict):
                    continue
                
                role = arg.get("role", "")
                argument = str(arg.get("argument", "")).strip()
                
                if role and argument:
                    triplets.append((event_type, role, argument))
        
        return triplets
    
    def extract_event_types(self, events: List[Dict]) -> Set[str]:
        """
        æå–äº‹ä»¶ç±»å‹é›†åˆ
        
        Args:
            events: äº‹ä»¶åˆ—è¡¨
        
        Returns:
            äº‹ä»¶ç±»å‹é›†åˆ
        """
        types = set()
        
        if not isinstance(events, list):
            return types
        
        for event in events:
            if isinstance(event, dict):
                etype = event.get("event_type", "")
                if etype:
                    types.add(etype)
        
        return types
    
    def relaxed_match(self, pred_arg: str, gold_arg: str) -> bool:
        """
        Relaxed æ¨¡å¼åŒ¹é…åˆ¤æ–­
        
        åˆ¤æ–­æ¡ä»¶ï¼ˆæ»¡è¶³å…¶ä¸€å³å¯ï¼‰:
        1. pred åŒ…å« gold
        2. gold åŒ…å« pred
        3. å­—ç¬¦çº§é‡å æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼
        
        Args:
            pred_arg: é¢„æµ‹çš„è®ºå…ƒå€¼
            gold_arg: æ ‡å‡†è®ºå…ƒå€¼
        
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        pred_norm = self.normalize_text(pred_arg)
        gold_norm = self.normalize_text(gold_arg)
        
        if not pred_norm or not gold_norm:
            return False
        
        # å®Œå…¨åŒ¹é…
        if pred_norm == gold_norm:
            return True
        
        # åŒ…å«å…³ç³»
        if pred_norm in gold_norm or gold_norm in pred_norm:
            return True
        
        # å­—ç¬¦çº§é‡å ï¼ˆJaccard-likeï¼‰
        pred_chars = set(pred_norm)
        gold_chars = set(gold_norm)
        
        if not pred_chars or not gold_chars:
            return False
        
        intersection = len(pred_chars & gold_chars)
        union = len(pred_chars | gold_chars)
        
        overlap = intersection / union if union > 0 else 0
        return overlap >= self.relaxed_threshold
    
    def compute_relaxed_matches(
        self, 
        pred_triplets: List[Tuple], 
        gold_triplets: List[Tuple]
    ) -> int:
        """
        è®¡ç®— Relaxed æ¨¡å¼çš„åŒ¹é…æ•°
        
        Args:
            pred_triplets: é¢„æµ‹ä¸‰å…ƒç»„åˆ—è¡¨
            gold_triplets: æ ‡å‡†ä¸‰å…ƒç»„åˆ—è¡¨
        
        Returns:
            åŒ¹é…æ•°ï¼ˆTrue Positivesï¼‰
        """
        matched_gold = set()  # è®°å½•å·²åŒ¹é…çš„ gold ç´¢å¼•ï¼Œé¿å…é‡å¤è®¡æ•°
        tp = 0
        
        for p_type, p_role, p_arg in pred_triplets:
            for g_idx, (g_type, g_role, g_arg) in enumerate(gold_triplets):
                if g_idx in matched_gold:
                    continue
                
                # ç±»å‹å’Œè§’è‰²å¿…é¡»å®Œå…¨åŒ¹é…
                if p_type != g_type or p_role != g_role:
                    continue
                
                # è®ºå…ƒä½¿ç”¨ Relaxed åŒ¹é…
                if self.relaxed_match(p_arg, g_arg):
                    tp += 1
                    matched_gold.add(g_idx)
                    break
        
        return tp
    
    def update(self, pred_events: List[Dict], gold_events: List[Dict], parse_success: bool = True):
        """
        æ›´æ–°è¯„ä¼°ç»Ÿè®¡
        
        Args:
            pred_events: é¢„æµ‹çš„äº‹ä»¶åˆ—è¡¨
            gold_events: æ ‡å‡†äº‹ä»¶åˆ—è¡¨
            parse_success: è§£ææ˜¯å¦æˆåŠŸ
        """
        self.stats["total_samples"] += 1
        
        if not parse_success:
            self.stats["parse_errors"] += 1
        
        # === Strict æ¨¡å¼ ===
        pred_strict = self.extract_triplets_strict(pred_events)
        gold_strict = self.extract_triplets_strict(gold_events)
        
        strict_tp = len(pred_strict & gold_strict)
        self.stats["strict_tp"] += strict_tp
        self.stats["strict_pred_total"] += len(pred_strict)
        self.stats["strict_gold_total"] += len(gold_strict)
        
        # === Relaxed æ¨¡å¼ ===
        pred_relaxed = self.extract_triplets_relaxed(pred_events)
        gold_relaxed = self.extract_triplets_relaxed(gold_events)
        
        relaxed_tp = self.compute_relaxed_matches(pred_relaxed, gold_relaxed)
        self.stats["relaxed_tp"] += relaxed_tp
        self.stats["relaxed_pred_total"] += len(pred_relaxed)
        self.stats["relaxed_gold_total"] += len(gold_relaxed)
        
        # === äº‹ä»¶ç±»å‹è¯†åˆ« ===
        pred_types = self.extract_event_types(pred_events)
        gold_types = self.extract_event_types(gold_events)
        
        type_tp = len(pred_types & gold_types)
        self.stats["type_tp"] += type_tp
        self.stats["type_pred_total"] += len(pred_types)
        self.stats["type_gold_total"] += len(gold_types)
        
        # === é”™è¯¯åˆ†æ ===
        if pred_strict != gold_strict:
            # æ¼æŠ¥ï¼ˆFalse Negativeï¼‰
            missed = gold_strict - pred_strict
            for m_type, m_role, _ in missed:
                self.stats["error_types"][f"FN_{m_type}_{m_role}"] += 1
            
            # è¯¯æŠ¥ï¼ˆFalse Positiveï¼‰
            spurious = pred_strict - gold_strict
            for s_type, s_role, _ in spurious:
                self.stats["error_types"][f"FP_{s_type}_{s_role}"] += 1
    
    def compute_metrics(self) -> MetricsReport:
        """
        è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆ2026 å­¦æœ¯è®ºæ–‡ç‰ˆï¼‰
        
        Returns:
            MetricsReport å¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡
        """
        report = MetricsReport()
        report.total_samples = self.stats["total_samples"]
        report.parse_errors = self.stats["parse_errors"]
        
        # è§£æé”™è¯¯ç‡
        if report.total_samples > 0:
            report.parse_error_rate = report.parse_errors / report.total_samples
        
        # === Strict F1 ===
        s_tp = self.stats["strict_tp"]
        s_pred = self.stats["strict_pred_total"]
        s_gold = self.stats["strict_gold_total"]
        
        report.strict_precision = s_tp / s_pred if s_pred > 0 else 0.0
        report.strict_recall = s_tp / s_gold if s_gold > 0 else 0.0
        if report.strict_precision + report.strict_recall > 0:
            report.strict_f1 = 2 * report.strict_precision * report.strict_recall / (report.strict_precision + report.strict_recall)
        
        # === Relaxed F1 ===
        r_tp = self.stats["relaxed_tp"]
        r_pred = self.stats["relaxed_pred_total"]
        r_gold = self.stats["relaxed_gold_total"]
        
        report.relaxed_precision = r_tp / r_pred if r_pred > 0 else 0.0
        report.relaxed_recall = r_tp / r_gold if r_gold > 0 else 0.0
        if report.relaxed_precision + report.relaxed_recall > 0:
            report.relaxed_f1 = 2 * report.relaxed_precision * report.relaxed_recall / (report.relaxed_precision + report.relaxed_recall)
        
        # === Type F1 ===
        t_tp = self.stats["type_tp"]
        t_pred = self.stats["type_pred_total"]
        t_gold = self.stats["type_gold_total"]
        
        report.type_precision = t_tp / t_pred if t_pred > 0 else 0.0
        report.type_recall = t_tp / t_gold if t_gold > 0 else 0.0
        if report.type_precision + report.type_recall > 0:
            report.type_f1 = 2 * report.type_precision * report.type_recall / (report.type_precision + report.type_recall)
        
        # === å¹»è§‰æ£€æµ‹æŒ‡æ ‡ ===
        if report.total_samples > 0:
            report.hallucination_rate = self.stats["hallucination_samples"] / report.total_samples
        if self.stats["total_entities"] > 0:
            report.hallucination_entity_rate = self.stats["hallucinated_entities"] / self.stats["total_entities"]
        
        # === CoT å¿ å®åº¦æŒ‡æ ‡ ===
        cot_checked = self.stats["cot_checked"]
        if cot_checked > 0:
            report.cot_faithfulness = self.stats["cot_fully_consistent"] / cot_checked
            report.cot_type_consistency = self.stats["cot_type_consistent"] / cot_checked
            report.cot_argument_consistency = self.stats["cot_argument_consistent"] / cot_checked
        
        # === Schema ç¬¦åˆåº¦ ===
        if report.total_samples > 0:
            report.schema_compliance_rate = self.stats["schema_compliant"] / report.total_samples
        
        # é”™è¯¯ç±»å‹åˆ†å¸ƒï¼ˆå– Top 10ï¼‰
        sorted_errors = sorted(
            self.stats["error_types"].items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:10]
        report.error_breakdown = dict(sorted_errors)
        
        return report
    
    def check_hallucination(self, source_text: str, pred_events: List[Dict]) -> Tuple[bool, int, int]:
        """
        æ£€æµ‹å¹»è§‰
        
        Args:
            source_text: åŸå§‹è¾“å…¥æ–‡æœ¬
            pred_events: é¢„æµ‹äº‹ä»¶åˆ—è¡¨
        
        Returns:
            (æ˜¯å¦æœ‰å¹»è§‰, å¹»è§‰å®ä½“æ•°, æ€»å®ä½“æ•°)
        """
        has_hallucination = False
        hallucinated_count = 0
        total_count = 0
        
        # æ¸…ç†åŸæ–‡
        clean_source = re.sub(r'\s+', '', source_text)
        
        if not isinstance(pred_events, list):
            return False, 0, 0
        
        for event in pred_events:
            if not isinstance(event, dict):
                continue
            
            for arg in event.get("arguments", []):
                if not isinstance(arg, dict):
                    continue
                
                argument = str(arg.get("argument", ""))
                if len(argument) < 2:
                    continue
                
                total_count += 1
                clean_arg = re.sub(r'\s+', '', argument)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨åŸæ–‡ä¸­
                if clean_arg not in clean_source:
                    has_hallucination = True
                    hallucinated_count += 1
        
        return has_hallucination, hallucinated_count, total_count
    
    def check_schema_compliance(
        self,
        pred_events: List[Dict],
        valid_event_types: Set[str] = None,
        valid_roles_by_event: Optional[Dict[str, Set[str]]] = None
    ) -> bool:
        """
        æ£€æµ‹ Schema ç¬¦åˆåº¦
        
        Args:
            pred_events: é¢„æµ‹äº‹ä»¶åˆ—è¡¨
            valid_event_types: æœ‰æ•ˆçš„äº‹ä»¶ç±»å‹é›†åˆ
            valid_roles_by_event:
                äº‹ä»¶ç±»å‹åˆ°åˆæ³•è§’è‰²é›†åˆçš„æ˜ å°„ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰ã€‚
                è‹¥æä¾›ï¼Œåˆ™æ¯ä¸ª argument.role å¿…é¡»å±äºå¯¹åº” event_type çš„åˆæ³•è§’è‰²é›†åˆã€‚
        
        Returns:
            æ˜¯å¦ç¬¦åˆ Schema
        """
        if not isinstance(pred_events, list):
            return False
        
        for event in pred_events:
            if not isinstance(event, dict):
                return False
            
            # å¿…é¡»æœ‰ event_type
            if "event_type" not in event:
                return False

            event_type = event["event_type"]
            # å¦‚æœæä¾›äº†æœ‰æ•ˆäº‹ä»¶ç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
            if valid_event_types and event_type not in valid_event_types:
                return False

            # å¿…é¡»æœ‰ arguments ä¸”ä¸ºåˆ—è¡¨
            if "arguments" not in event or not isinstance(event.get("arguments"), list):
                return False

            allowed_roles = None
            if valid_roles_by_event is not None:
                allowed_roles = valid_roles_by_event.get(event_type)
                # æä¾›äº† role schema æ—¶ï¼ŒæœªçŸ¥äº‹ä»¶ç±»å‹ä¹Ÿè§†ä¸ºä¸åˆè§„
                if allowed_roles is None:
                    return False

            # æ¯ä¸ª argument å¿…é¡»æœ‰ role å’Œ argument å­—æ®µ
            for arg in event["arguments"]:
                if not isinstance(arg, dict):
                    return False
                if "role" not in arg or "argument" not in arg:
                    return False
                if allowed_roles is not None and arg.get("role") not in allowed_roles:
                    return False
        
        return True
    
    def update_with_extended_metrics(
        self, 
        pred_events: List[Dict], 
        gold_events: List[Dict], 
        source_text: str = "",
        full_response: str = "",
        parse_success: bool = True,
        valid_event_types: Set[str] = None,
        valid_roles_by_event: Optional[Dict[str, Set[str]]] = None
    ):
        """
        ä½¿ç”¨æ‰©å±•æŒ‡æ ‡æ›´æ–°ç»Ÿè®¡
        
        è¿™æ˜¯ update() æ–¹æ³•çš„æ‰©å±•ç‰ˆæœ¬ï¼Œæ”¯æŒå¹»è§‰æ£€æµ‹å’Œ CoT å¿ å®åº¦æ£€æµ‹
        """
        # è°ƒç”¨åŸæœ‰çš„ update
        self.update(pred_events, gold_events, parse_success)
        
        # === å¹»è§‰æ£€æµ‹ ===
        if source_text:
            has_halluc, halluc_count, total_entities = self.check_hallucination(source_text, pred_events)
            if has_halluc:
                self.stats["hallucination_samples"] += 1
            self.stats["hallucinated_entities"] += halluc_count
            self.stats["total_entities"] += total_entities
        
        # === Schema ç¬¦åˆåº¦ ===
        if self.check_schema_compliance(
            pred_events,
            valid_event_types=valid_event_types,
            valid_roles_by_event=valid_roles_by_event,
        ):
            self.stats["schema_compliant"] += 1
        
        # === CoT å¿ å®åº¦æ£€æµ‹ ===
        if full_response and ("<thought>" in full_response or "```json" in full_response):
            self.stats["cot_checked"] += 1
            
            # ç®€åŒ–çš„ CoT ä¸€è‡´æ€§æ£€æµ‹
            cot_result = self._check_cot_consistency(full_response, pred_events)
            if cot_result["type_consistent"]:
                self.stats["cot_type_consistent"] += 1
            if cot_result["argument_consistent"]:
                self.stats["cot_argument_consistent"] += 1
            if cot_result["fully_consistent"]:
                self.stats["cot_fully_consistent"] += 1
    
    def _check_cot_consistency(self, full_response: str, pred_events: List[Dict]) -> Dict:
        """
        å†…éƒ¨æ–¹æ³•ï¼šæ£€æµ‹ CoT ä¸ JSON è¾“å‡ºçš„ä¸€è‡´æ€§
        """
        result = {
            "type_consistent": True,
            "argument_consistent": True,
            "fully_consistent": True
        }
        
        # æå– thought éƒ¨åˆ†
        thought_match = re.search(r'<thought>(.*?)</thought>', full_response, re.DOTALL)
        if not thought_match:
            # æ²¡æœ‰ thought æ ‡ç­¾ï¼Œå– json ä¹‹å‰çš„å†…å®¹
            json_start = full_response.find("```json")
            if json_start > 0:
                thought_text = full_response[:json_start]
            else:
                return result  # æ— æ³•æ£€æµ‹
        else:
            thought_text = thought_match.group(1)
        
        # æå– JSON ä¸­çš„äº‹ä»¶ç±»å‹
        json_event_types = set()
        if isinstance(pred_events, list):
            for event in pred_events:
                if isinstance(event, dict) and event.get("event_type"):
                    json_event_types.add(event["event_type"])
        
        # æ£€æµ‹äº‹ä»¶ç±»å‹æ˜¯å¦åœ¨ thought ä¸­è¢«æåŠ
        for etype in json_event_types:
            if etype not in thought_text:
                result["type_consistent"] = False
                result["fully_consistent"] = False
                break
        
        return result


# ===========================
# 3. è¯„ä¼°è„šæœ¬ä¸»é€»è¾‘
# ===========================

DEFAULT_EVAL_PROTOCOL: Dict[str, Any] = {
    "version": "1.0",
    "primary_metric": "strict_f1",
    "canonical_metric_mode": "analysis_only",
    "evaluation": {
        "split": "dev",
        "seeds": [3407, 3408, 3409],
        "bootstrap_samples": 1000,
        "concurrency": 8,
        "significance": "paired_permutation",
        "confidence": 0.95,
    },
}


def _deep_merge_dict(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    merged = copy.deepcopy(base)
    for key, value in (override or {}).items():
        if isinstance(value, dict) and isinstance(merged.get(key), dict):
            merged[key] = _deep_merge_dict(merged[key], value)
        else:
            merged[key] = value
    return merged


def load_eval_protocol(path: Optional[str]) -> Dict[str, Any]:
    if not path or not os.path.exists(path):
        return copy.deepcopy(DEFAULT_EVAL_PROTOCOL)
    with open(path, "r", encoding="utf-8") as f:
        payload = yaml.safe_load(f) or {}
    if not isinstance(payload, dict):
        raise ValueError(f"åè®®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ˆéœ€ä¸º dictï¼‰: {path}")
    return _deep_merge_dict(DEFAULT_EVAL_PROTOCOL, payload)


def load_role_alias_map(path: Optional[str]) -> Dict[str, Dict[str, str]]:
    if not path or not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        if str(path).lower().endswith(".json"):
            payload = json.load(f)
        else:
            payload = yaml.safe_load(f)
    if not isinstance(payload, dict):
        return {}
    root = payload.get("event_role_aliases", payload)
    if not isinstance(root, dict):
        return {}

    normalized: Dict[str, Dict[str, str]] = {}
    for event_type, mapping in root.items():
        if not isinstance(mapping, dict):
            continue
        event_key = str(event_type)
        normalized[event_key] = {}
        for alias, canonical in mapping.items():
            if not alias or not canonical:
                continue
            normalized[event_key][str(alias)] = str(canonical)
    return normalized


def canonicalize_pred_roles(
    pred_events: List[Dict[str, Any]],
    alias_map: Dict[str, Dict[str, str]],
) -> Tuple[List[Dict[str, Any]], int]:
    if not isinstance(pred_events, list) or not alias_map:
        return pred_events if isinstance(pred_events, list) else [], 0

    rewritten = 0
    normalized_events: List[Dict[str, Any]] = []
    for event in pred_events:
        if not isinstance(event, dict):
            continue
        event_type = event.get("event_type")
        role_map = alias_map.get(str(event_type), {}) if event_type else {}
        new_event = dict(event)
        args = event.get("arguments", [])
        if isinstance(args, list):
            new_args: List[Dict[str, Any]] = []
            for arg in args:
                if not isinstance(arg, dict):
                    continue
                new_arg = dict(arg)
                role = new_arg.get("role")
                if isinstance(role, str) and role in role_map:
                    mapped = role_map[role]
                    if mapped != role:
                        rewritten += 1
                    new_arg["role"] = mapped
                new_args.append(new_arg)
            new_event["arguments"] = new_args
        normalized_events.append(new_event)
    return normalized_events, rewritten


def safe_compute_file_sha256(path: Optional[str]) -> Optional[str]:
    if not path or not os.path.exists(path):
        return None
    return compute_file_sha256(path)


def parse_args():
    parser = argparse.ArgumentParser(description="OG-LANS è¯„ä¼°è„šæœ¬")
    parser.add_argument("--config", type=str, default="configs/config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--protocol",
        type=str,
        default="configs/eval_protocol.yaml",
        help="è¯„ä¼°åè®®æ–‡ä»¶ï¼ˆä¸»æŒ‡æ ‡ä¸ç»Ÿè®¡è§„èŒƒï¼‰",
    )
    parser.add_argument("--checkpoint", type=str, required=True, help="LoRA checkpoint è·¯å¾„")
    parser.add_argument("--seed", type=int, default=3407, help="éšæœºç§å­ï¼ˆå¤ç°æ€§ï¼‰")
    parser.add_argument("--num_samples", type=int, default=None, help="è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆNone=å…¨éƒ¨ï¼‰")
    parser.add_argument("--batch_size", type=int, default=4, help="æ¨ç†æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--split", type=str, default="dev", help="æ•°æ®é›†åˆ’åˆ† (train/dev/test)")
    parser.add_argument("--output_file", type=str, default="eval_results.jsonl", help="ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--eval_mode", type=str, default="both", choices=["strict", "relaxed", "both"], 
                        help="è¯„ä¼°æ¨¡å¼: strict/relaxed/both")
    parser.add_argument("--use_oneshot", action="store_true", help="ä½¿ç”¨ One-Shot ç¤ºä¾‹è¿›è¡Œæ¨ç†")
    parser.add_argument("--verbose", action="store_true", help="è¾“å‡ºè¯¦ç»†æ—¥å¿—")
    parser.add_argument("--do_sample", action="store_true", default=False,
                        help="ä½¿ç”¨é‡‡æ ·è§£ç ï¼ˆé»˜è®¤ Falseï¼Œä½¿ç”¨ greedy ç¡®å®šæ€§è§£ç ï¼‰")
    parser.add_argument(
        "--role_alias_map",
        type=str,
        default="configs/role_aliases_duee_fin.yaml",
        help="è§’è‰²åˆ«åæ˜ å°„æ–‡ä»¶ï¼ˆç”¨äºè¾…åŠ© canonical æŒ‡æ ‡ï¼‰",
    )
    parser.add_argument(
        "--canonical_metric_mode",
        type=str,
        default=None,
        choices=["off", "analysis_only", "apply_for_aux_metric"],
        help="canonical æŒ‡æ ‡æ¨¡å¼ï¼šoff / analysis_only / apply_for_aux_metric",
    )
    parser.add_argument(
        "--report_primary_metric",
        type=str,
        default=None,
        help="ä¸»æŠ¥å‘ŠæŒ‡æ ‡åï¼ˆé»˜è®¤è¯»å– protocol.primary_metricï¼‰",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="æ¨ç†è®¾å¤‡é€‰æ‹©ï¼ˆauto/cuda/cpuï¼‰",
    )
    return parser.parse_args()


def get_local_model_path(m_cfg: dict) -> str:
    """è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ”¯æŒ ModelScope ä¸‹è½½ï¼‰"""
    model_name_or_path = m_cfg['base_model']
    if m_cfg.get('source') == 'modelscope':
        try:
            from modelscope import snapshot_download
            model_name_or_path = snapshot_download(model_name_or_path, cache_dir='./models')
        except Exception as e:
            print(f"âš ï¸ ModelScope ä¸‹è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„")
    return model_name_or_path


def print_metrics_report(report: MetricsReport, eval_mode: str = "both"):
    """æ‰“å°æ ¼å¼åŒ–çš„è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š OG-LANS è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    print(f"\nğŸ“ˆ æ ·æœ¬ç»Ÿè®¡")
    print(f"   æ€»æ ·æœ¬æ•°: {report.total_samples}")
    print(f"   è§£æå¤±è´¥: {report.parse_errors} ({report.parse_error_rate:.2%})")
    
    if eval_mode in ["strict", "both"]:
        print(f"\nğŸ“ Strict æ¨¡å¼ (å®Œå…¨åŒ¹é…)")
        print(f"   Precision: {report.strict_precision:.4f}")
        print(f"   Recall:    {report.strict_recall:.4f}")
        print(f"   F1 Score:  {report.strict_f1:.4f}")
    
    if eval_mode in ["relaxed", "both"]:
        print(f"\nğŸ“ Relaxed æ¨¡å¼ (éƒ¨åˆ†åŒ¹é…)")
        print(f"   Precision: {report.relaxed_precision:.4f}")
        print(f"   Recall:    {report.relaxed_recall:.4f}")
        print(f"   F1 Score:  {report.relaxed_f1:.4f}")
    
    print(f"\nğŸ·ï¸ äº‹ä»¶ç±»å‹è¯†åˆ«")
    print(f"   Type Precision: {report.type_precision:.4f}")
    print(f"   Type Recall:    {report.type_recall:.4f}")
    print(f"   Type F1 Score:  {report.type_f1:.4f}")
    
    if report.error_breakdown:
        print(f"\nâŒ ä¸»è¦é”™è¯¯ç±»å‹ (Top 10)")
        for error_type, count in report.error_breakdown.items():
            print(f"   {error_type}: {count}")
    
    # å¹»è§‰æ£€æµ‹å’Œ CoT å¿ å®åº¦æŒ‡æ ‡
    print(f"\nğŸ”® é«˜çº§æŒ‡æ ‡")
    print(f"   å¹»è§‰æ ·æœ¬ç‡:      {report.hallucination_rate:.4f}")
    print(f"   å¹»è§‰å®ä½“ç‡:      {report.hallucination_entity_rate:.4f}")
    print(f"   CoT å¿ å®åº¦:      {report.cot_faithfulness:.4f}")
    print(f"   CoT ç±»å‹ä¸€è‡´æ€§:  {report.cot_type_consistency:.4f}")
    print(f"   CoT è®ºå…ƒä¸€è‡´æ€§:  {report.cot_argument_consistency:.4f}")
    print(f"   Schema ç¬¦åˆç‡(ç±»å‹+è§’è‰²): {report.schema_compliance_rate:.4f}")
    
    print("\n" + "=" * 60)


def main():
    # ä»…åœ¨æœ¬åœ°è¯„ä¼°æ‰§è¡Œæ—¶åŠ è½½æ·±åº¦å­¦ä¹ ä¾èµ–ï¼Œé¿å… API-only ç¯å¢ƒçš„ç¡¬ä¾èµ–é—®é¢˜
    try:
        import numpy as np
        import torch
    except Exception as e:
        raise RuntimeError(
            "æœ¬åœ°æ¨¡å‹è¯„ä¼°ä¾èµ– numpy/torchã€‚è‹¥åªéœ€ API è¯„ä¼°ï¼Œè¯·ä½¿ç”¨ evaluate_api.pyã€‚"
        ) from e

    try:
        from unsloth import FastLanguageModel
    except Exception as e:
        raise RuntimeError(
            "æœ¬åœ°æ¨¡å‹è¯„ä¼°ä¾èµ– unslothã€‚è‹¥åªéœ€ API è¯„ä¼°ï¼Œè¯·ä½¿ç”¨ evaluate_api.pyã€‚"
        ) from e

    args = parse_args()
    run_start_ts = time.time()
    cmdline = " ".join(os.sys.argv)

    # 0. å¤ç°æ€§è®¾ç½®
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    # ä¿å®ˆè®¾ç½®ï¼šä¼˜å…ˆå¯å¤ç°è€Œéæè‡´é€Ÿåº¦
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    elif args.device == "cuda":
        if not torch.cuda.is_available():
            raise RuntimeError("å‚æ•° --device cuda ä½†å½“å‰ç¯å¢ƒä¸å¯ç”¨ CUDAã€‚")
        device = "cuda"
    else:
        device = "cpu"

    # 1. åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    protocol = load_eval_protocol(args.protocol)
    if args.report_primary_metric is None:
        args.report_primary_metric = str(protocol.get("primary_metric", "strict_f1"))
    if args.canonical_metric_mode is None:
        args.canonical_metric_mode = str(protocol.get("canonical_metric_mode", "analysis_only"))
    if args.canonical_metric_mode not in {"off", "analysis_only", "apply_for_aux_metric"}:
        raise ValueError(f"Unsupported canonical metric mode: {args.canonical_metric_mode}")

    # 2. è·¯å¾„è§£æ
    checkpoint_path = os.path.normpath(args.checkpoint)
    try:
        path_parts = checkpoint_path.split(os.sep)
        idx = path_parts.index("checkpoints")
        dataset_name = path_parts[idx - 1]
        # Fix: debug directory is not a dataset name, use default.
        if dataset_name == "debug":
            dataset_name = "DuEE-Fin"
    except (ValueError, IndexError):
        dataset_name = "DuEE-Fin"

    dataset_name_lower = dataset_name.lower().replace("-", "_")
    schema_path = f"./data/raw/{dataset_name}/{dataset_name_lower}_event_schema.json"
    data_path = f"./data/raw/{dataset_name}"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆæ¯æ¬¡è¿è¡Œç‹¬ç«‹ç›®å½•ï¼Œé¿å…è¦†ç›–ï¼‰
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    run_id = f"{timestamp}_{args.split}_seed{args.seed}_p{os.getpid()}"
    eval_output_dir = f"./logs/{dataset_name}/eval_local/{run_id}"
    os.makedirs(eval_output_dir, exist_ok=True)

    if args.output_file == "eval_results.jsonl":
        final_output_path = os.path.join(eval_output_dir, "eval_results.jsonl")
    elif not os.path.dirname(args.output_file):
        final_output_path = os.path.join(eval_output_dir, args.output_file)
    else:
        final_output_path = args.output_file
        os.makedirs(os.path.dirname(final_output_path), exist_ok=True)

    artifact_dir = os.path.dirname(final_output_path) or "."
    os.makedirs(artifact_dir, exist_ok=True)
    run_manifest_path = os.path.join(artifact_dir, "run_manifest.json")
    repo_dir = os.path.dirname(os.path.abspath(__file__))
    runtime_manifest = collect_runtime_manifest(
        repo_dir,
        package_names=["torch", "transformers", "trl", "unsloth", "dirtyjson", "PyYAML"],
    )
    config_hash = compute_file_sha256(args.config)

    print(f"ğŸ“Š æ•°æ®é›†: {dataset_name} | åˆ’åˆ†: {args.split}")
    print(f"ğŸ“‚ Schema: {schema_path}")
    print(f"ğŸ†” Run ID: {run_id}")
    print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {final_output_path}")
    print(f"ğŸ“œ Protocol: {args.protocol}")
    print(f"ğŸ¯ Primary Metric: {args.report_primary_metric}")
    print(f"ğŸ§­ Canonical Metric Mode: {args.canonical_metric_mode}")

    # 3. åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    base_model_path = get_local_model_path(config['model'])
    load_in_4bit = config['model'].get('load_in_4bit', True)
    if device == "cpu" and load_in_4bit:
        # bitsandbytes 4bit åœ¨ CPU è·¯å¾„é€šå¸¸ä¸å¯ç”¨ï¼Œæ˜¾å¼é™çº§ä¸ºé 4bit ä»¥é¿å…ç›´æ¥å´©æºƒ
        print("âš ï¸ æ£€æµ‹åˆ° CPU æ¨ç†ï¼Œè‡ªåŠ¨ç¦ç”¨ load_in_4bitï¼ˆåŸé…ç½®ä¸º Trueï¼‰ã€‚")
        load_in_4bit = False

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=base_model_path,
        max_seq_length=config['model'].get('max_seq_length', 4096),
        load_in_4bit=load_in_4bit,
    )
    # [ä¿®å¤] æ­£ç¡®çš„åŠ è½½é¡ºåºï¼šå…ˆåŠ è½½ adapterï¼Œå†åˆ‡æ¢æ¨ç†æ¨¡å¼
    model.load_adapter(args.checkpoint)
    FastLanguageModel.for_inference(model)
    model_quantized = bool(load_in_4bit) or is_quantized_model(model)
    if model_quantized:
        model_device_strategy = "auto_from_pretrained"
        print("â„¹ï¸ æ£€æµ‹åˆ°é‡åŒ–æ¨¡å‹ï¼Œè·³è¿‡ model.to(device)ï¼ˆç”± from_pretrained è‡ªåŠ¨æ”¾ç½®è®¾å¤‡ï¼‰ã€‚")
    else:
        model_device_strategy = "manual_to_device"
        model.to(device)
    model.eval()  # æ˜¾å¼è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print(f"ğŸ–¥ï¸ æ¨ç†è®¾å¤‡: {device}")
    
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # [ä¿®å¤] æ˜¾å¼æ£€æŸ¥ EOS tokenï¼ˆQwen3 é€šå¸¸ä½¿ç”¨ <|im_end|>ï¼‰
    expected_eos = "<|im_end|>"
    if tokenizer.eos_token is None or tokenizer.eos_token == "":
        tokenizer.eos_token = expected_eos
    elif tokenizer.eos_token != expected_eos:
        print(f"âš ï¸ EOS token ä¸º {tokenizer.eos_token}ï¼ŒæœŸæœ› {expected_eos}ã€‚å°†ä¿ç•™å½“å‰è®¾ç½®ã€‚")
    print(f"ğŸ”§ EOS Token: {tokenizer.eos_token} | EOS Token ID: {tokenizer.eos_token_id}")

    # 4. åŠ è½½æ•°æ®
    print("\nğŸ“š åŠ è½½æ•°æ®...")
    adapter = DuEEFinAdapter(data_path=data_path, schema_path=schema_path)
    try:
        all_samples = adapter.load_data(args.split)
    except Exception as e:
        # ã€å…³é”®ä¿®å¤ã€‘ä¸å†è‡ªåŠ¨ fallback åˆ°è®­ç»ƒé›†ï¼Œé¿å…è¯„ä¼°æŒ‡æ ‡è™šé«˜
        print(f"âŒ åŠ è½½ {args.split} æ•°æ®é›†å¤±è´¥: {e}")
        print(f"   è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œ split å‚æ•°æ˜¯å¦æ­£ç¡®")
        print(f"   å¯ç”¨çš„ split é€‰é¡¹: train, dev, test")
        raise RuntimeError(f"æ— æ³•åŠ è½½ {args.split} æ•°æ®é›†ï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨") from e

    if args.num_samples:
        all_samples = all_samples[:args.num_samples]

    print(f"   åŠ è½½ {len(all_samples)} æ¡æ ·æœ¬")

    # 5. åˆå§‹åŒ–è¯„ä¼°å™¨å’Œè§£æå™¨
    evaluator = AcademicEventEvaluator()
    role_alias_map = load_role_alias_map(args.role_alias_map)
    canonical_enabled = bool(args.canonical_metric_mode != "off" and role_alias_map)
    canonical_evaluator = AcademicEventEvaluator() if canonical_enabled else None
    canonical_rewrites_total = 0
    json_parser = RobustJSONParser()

    results_to_save = []

    # 6. æ‰¹é‡æ¨ç†
    decoding_strategy = "é‡‡æ ·è§£ç  (Sampling)" if args.do_sample else "ç¡®å®šæ€§è§£ç  (Greedy)"
    print(f"\nğŸš€ å¼€å§‹æ¨ç† (Batch Size: {args.batch_size}, è§£ç ç­–ç•¥: {decoding_strategy})...")
    pbar = tqdm(range(0, len(all_samples), args.batch_size), desc="è¯„ä¼°è¿›åº¦")

    for i in pbar:
        batch_samples = all_samples[i:i + args.batch_size]
        batch_prompts = []

        for sample in batch_samples:
            # [ä¿®å¤] ä½¿ç”¨ç»Ÿä¸€çš„ prompt æ„å»ºå‡½æ•°ï¼Œç¡®ä¿è®­ç»ƒ/è¯„ä¼°ä¸€è‡´æ€§
            formatted_prompt = build_inference_prompt(
                text=sample.text,
                tokenizer=tokenizer,
                use_oneshot=args.use_oneshot,
                schema=getattr(adapter, "schema", None),
            )
            batch_prompts.append(formatted_prompt)

        # Tokenize
        inputs = tokenizer(
            batch_prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=config['model'].get('max_seq_length', 4096)
        ).to(device)
        
        # æ¨ç†
        with torch.no_grad():
            # [ä¿®å¤] è·å– inference é…ç½®èŠ‚ç‚¹ï¼ˆç›´æ¥è·å–ï¼Œä¸è¦åŠ  ['parameters']ï¼‰
            inf_cfg = config.get('inference', {})

            # æ„å»ºç”Ÿæˆå‚æ•°
            generate_kwargs = {
                "max_new_tokens": inf_cfg.get('max_new_tokens', 2048),
                "use_cache": True,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
            }

            # æ ¹æ® do_sample å‚æ•°é€‰æ‹©è§£ç ç­–ç•¥
            if args.do_sample:
                # é‡‡æ ·è§£ç ï¼šä½¿ç”¨é…ç½®ä¸­çš„æ¸©åº¦å’Œé‡‡æ ·å‚æ•°
                generate_kwargs.update({
                    "do_sample": True,
                    "temperature": inf_cfg.get('temperature', 0.7),
                    "top_p": inf_cfg.get('top_p', 0.8),
                    "top_k": inf_cfg.get('top_k', 20),
                })
            else:
                # ç¡®å®šæ€§è§£ç ï¼ˆGreedyï¼‰ï¼šä¸ä¼ é‡‡æ ·å‚æ•°
                generate_kwargs["do_sample"] = False

            outputs = model.generate(**inputs, **generate_kwargs)
        
        # è§£ç 
        generated_ids = outputs[:, inputs.input_ids.shape[1]:]
        decoded_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for j, response in enumerate(decoded_responses):
            sample = batch_samples[j]
            
            # ä½¿ç”¨é²æ£’è§£æå™¨
            pred_events, parse_diagnostics = json_parser.parse(response)
            parse_success = parse_diagnostics.get("success", False)
            
            if pred_events is None:
                pred_events = []
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨
            if isinstance(pred_events, dict):
                pred_events = [pred_events]
            
            # è§£æ Ground Truth
            # [ä¿®å¤] ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„ events å­—æ®µï¼Œé¿å…è§£æå¤±è´¥å½±å“æŒ‡æ ‡
            if hasattr(sample, 'events') and sample.events:
                gold_events = sample.events
            else:
                gold_events, _ = json_parser.parse(sample.chosen)
                if gold_events is None:
                    gold_events = []
                if isinstance(gold_events, dict):
                    gold_events = [gold_events]
            
            # ä½¿ç”¨æ‰©å±•ç‰ˆè¯„ä¼°æ–¹æ³•ï¼Œæ”¯æŒå¹»è§‰æ£€æµ‹ã€CoT å¿ å®åº¦å’Œä¸¥æ ¼ Schema æ ¡éªŒ
            valid_types = set(adapter.get_event_types()) if hasattr(adapter, 'get_event_types') else None
            valid_roles_by_event = None
            if hasattr(adapter, 'schema') and isinstance(adapter.schema, dict):
                valid_roles_by_event = {
                    etype: set(roles or [])
                    for etype, roles in adapter.schema.items()
                }
            evaluator.update_with_extended_metrics(
                pred_events=pred_events, 
                gold_events=gold_events, 
                source_text=sample.text,
                full_response=response,
                parse_success=parse_success,
                valid_event_types=valid_types,
                valid_roles_by_event=valid_roles_by_event
            )
            canonical_pred_events = pred_events
            rewrite_count = 0
            if canonical_evaluator is not None:
                canonical_pred_events, rewrite_count = canonicalize_pred_roles(pred_events, role_alias_map)
                canonical_rewrites_total += rewrite_count
                canonical_evaluator.update_with_extended_metrics(
                    pred_events=canonical_pred_events,
                    gold_events=gold_events,
                    source_text=sample.text,
                    full_response=response,
                    parse_success=parse_success,
                    valid_event_types=valid_types,
                    valid_roles_by_event=valid_roles_by_event,
                )
            
            # ä¿å­˜ç»“æœ
            results_to_save.append({
                "id": sample.id,
                "text_preview": sample.text[:200] + "..." if len(sample.text) > 200 else sample.text,
                "ground_truth": gold_events,
                "prediction": pred_events,
                "prediction_canonical": canonical_pred_events if canonical_enabled else None,
                "canonical_role_rewrites": rewrite_count if canonical_enabled else 0,
                "raw_response": response[:1000] if len(response) > 1000 else response,
                "parse_success": parse_success,
                "parse_method": parse_diagnostics.get("extraction_method", "unknown"),
                "repair_steps": parse_diagnostics.get("repair_steps", [])
            })
            
            # è¯¦ç»†æ—¥å¿—
            if args.verbose and not parse_success:
                print(f"\nâš ï¸ æ ·æœ¬ {sample.id} è§£æå¤±è´¥")
                print(f"   æ–¹æ³•: {parse_diagnostics.get('extraction_method')}")
                print(f"   é”™è¯¯: {parse_diagnostics.get('error', 'Unknown')}")

    # 7. è®¡ç®—æŒ‡æ ‡å¹¶è¾“å‡ºæŠ¥å‘Š
    report = evaluator.compute_metrics()
    print_metrics_report(report, args.eval_mode)

    # 8. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    with open(final_output_path, 'w', encoding='utf-8') as f:
        for res in results_to_save:
            f.write(json.dumps(res, ensure_ascii=False) + "\n")

    parse_success = report.total_samples - report.parse_errors
    parse_success_rate = (parse_success / report.total_samples) if report.total_samples > 0 else 0.0
    canonical_report = canonical_evaluator.compute_metrics() if canonical_evaluator is not None else None

    # å…¼å®¹æ—§ç‰ˆæŒ‡æ ‡æ–‡ä»¶ç»“æ„ï¼ˆä¿ç•™ï¼‰
    metrics_file = final_output_path.replace(".jsonl", "_metrics.json")
    metrics_dict = {
        "_meta": {
            "project": "OG-LANS",
            "run_id": run_id,
            "timestamp": timestamp,
            "dataset": dataset_name,
            "split": args.split,
            "seed": args.seed,
            "command": cmdline,
            "config_path": os.path.abspath(args.config),
            "config_hash_sha256": config_hash,
            "protocol_path": os.path.abspath(args.protocol) if args.protocol else None,
            "protocol_hash_sha256": safe_compute_file_sha256(args.protocol),
            "protocol_version": protocol.get("version"),
            "primary_metric": args.report_primary_metric,
            "canonical_metric_mode": args.canonical_metric_mode,
            "role_alias_map_path": os.path.abspath(args.role_alias_map) if args.role_alias_map else None,
            "role_alias_map_hash_sha256": safe_compute_file_sha256(args.role_alias_map),
            "checkpoint": os.path.abspath(args.checkpoint),
            "output_file": os.path.abspath(final_output_path),
            "runtime_manifest": runtime_manifest,
        },
        "strict": {
            "precision": round(report.strict_precision, 4),
            "recall": round(report.strict_recall, 4),
            "f1": round(report.strict_f1, 4)
        },
        "relaxed": {
            "precision": round(report.relaxed_precision, 4),
            "recall": round(report.relaxed_recall, 4),
            "f1": round(report.relaxed_f1, 4)
        },
        "type_identification": {
            "precision": round(report.type_precision, 4),
            "recall": round(report.type_recall, 4),
            "f1": round(report.type_f1, 4)
        },
        "parse_statistics": {
            "total_samples": report.total_samples,
            "parse_errors": report.parse_errors,
            "parse_error_rate": round(report.parse_error_rate, 4),
            "parse_success_rate": round(parse_success_rate, 4),
        },
        "hallucination": {
            "sample_rate": round(report.hallucination_rate, 4),
            "entity_rate": round(report.hallucination_entity_rate, 4)
        },
        "cot_faithfulness": {
            "overall": round(report.cot_faithfulness, 4),
            "type_consistency": round(report.cot_type_consistency, 4),
            "argument_consistency": round(report.cot_argument_consistency, 4)
        },
        "schema_compliance_rate": round(report.schema_compliance_rate, 4),
        "error_breakdown": report.error_breakdown,
        "primary_metric": args.report_primary_metric,
        "primary_metric_value": round(float({
            "strict_f1": report.strict_f1,
            "relaxed_f1": report.relaxed_f1,
            "type_f1": report.type_f1,
        }.get(args.report_primary_metric, report.strict_f1)), 4),
    }
    if canonical_report is not None:
        metrics_dict["auxiliary_metrics"] = {
            "canonicalized": {
                "strict_precision": round(canonical_report.strict_precision, 4),
                "strict_recall": round(canonical_report.strict_recall, 4),
                "strict_f1": round(canonical_report.strict_f1, 4),
                "relaxed_precision": round(canonical_report.relaxed_precision, 4),
                "relaxed_recall": round(canonical_report.relaxed_recall, 4),
                "relaxed_f1": round(canonical_report.relaxed_f1, 4),
                "type_precision": round(canonical_report.type_precision, 4),
                "type_recall": round(canonical_report.type_recall, 4),
                "type_f1": round(canonical_report.type_f1, 4),
                "schema_compliance_rate": round(canonical_report.schema_compliance_rate, 4),
                "canonical_role_rewrites_total": canonical_rewrites_total,
                "canonical_role_rewrites_avg": round(
                    canonical_rewrites_total / report.total_samples if report.total_samples else 0.0,
                    4,
                ),
            }
        }
    save_json(metrics_file, metrics_dict)

    # æ–°ç‰ˆç»Ÿä¸€æ‘˜è¦ç»“æ„ï¼ˆä¸ evaluate_api.py å¯¹é½ï¼‰
    summary_file = final_output_path.replace(".jsonl", "_summary.json")
    eval_summary = {
        "meta": {
            "run_id": run_id,
            "run_dir": os.path.abspath(artifact_dir),
            "timestamp": timestamp,
            "model": config.get("model", {}).get("base_model"),
            "api_response_models": [],
            "dataset": dataset_name,
            "num_samples": report.total_samples,
            "split": args.split,
            "concurrency": None,
            "has_gold_labels": True,
            "use_fewshot": bool(args.use_oneshot),
            "fewshot_num_examples": 1 if args.use_oneshot else 0,
            "prompt_style": "qwen",
            "json_mode": "off",
            "seed": args.seed,
            "config_hash_sha256": config_hash,
            "config_path": os.path.abspath(args.config),
            "command": cmdline,
            "bootstrap_samples": None,
            "compute_ci": False,
            "protocol_path": os.path.abspath(args.protocol) if args.protocol else None,
            "protocol_hash_sha256": safe_compute_file_sha256(args.protocol),
            "protocol_version": protocol.get("version"),
            "primary_metric": args.report_primary_metric,
            "canonical_metric_mode": args.canonical_metric_mode,
            "role_alias_map_path": os.path.abspath(args.role_alias_map) if args.role_alias_map else None,
            "role_alias_map_hash_sha256": safe_compute_file_sha256(args.role_alias_map),
            "role_alias_map_loaded": bool(role_alias_map),
            "metrics_report_file": None,
            "log_file": None,
            "generation": {
                "temperature": config.get("inference", {}).get("temperature", 0.7) if args.do_sample else 0.0,
                "max_tokens": config.get("inference", {}).get("max_new_tokens", 2048),
                "max_retries": None,
                "json_mode": "off",
                "do_sample": bool(args.do_sample),
                "batch_size": args.batch_size,
            },
            "model_quantized": bool(model_quantized),
            "model_device_strategy": model_device_strategy,
            "model_target_device": device,
            "prompt_hashes": {},
            "checkpoint": os.path.abspath(args.checkpoint),
        },
        "metrics": {
            "strict_precision": round(report.strict_precision, 4),
            "strict_recall": round(report.strict_recall, 4),
            "strict_f1": round(report.strict_f1, 4),
            "relaxed_precision": round(report.relaxed_precision, 4),
            "relaxed_recall": round(report.relaxed_recall, 4),
            "relaxed_f1": round(report.relaxed_f1, 4),
            "type_precision": round(report.type_precision, 4),
            "type_recall": round(report.type_recall, 4),
            "type_f1": round(report.type_f1, 4),
            "parse_success": parse_success,
            "parse_failure": report.parse_errors,
            "parse_success_rate": round(parse_success_rate, 4),
            "hallucination_rate": round(report.hallucination_rate, 4),
            "hallucination_entity_rate": round(report.hallucination_entity_rate, 4),
            "cot_faithfulness": round(report.cot_faithfulness, 4),
            "cot_type_consistency": round(report.cot_type_consistency, 4),
            "cot_argument_consistency": round(report.cot_argument_consistency, 4),
            "schema_compliance_rate": round(report.schema_compliance_rate, 4),
            "error_breakdown": report.error_breakdown,
            "bootstrap_ci": None,
            "primary_metric": args.report_primary_metric,
            "primary_metric_value": round(float({
                "strict_f1": report.strict_f1,
                "relaxed_f1": report.relaxed_f1,
                "type_f1": report.type_f1,
            }.get(args.report_primary_metric, report.strict_f1)), 4),
        },
        "token_usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "avg_tokens_per_sample": 0.0,
        },
        "api_stats": {
            "failed_calls": 0,
            "failed_call_rate": 0.0,
        },
        "runtime": {
            "wall_clock_seconds": round(time.time() - run_start_ts, 4),
        },
        "runtime_manifest": runtime_manifest,
        "analysis": {
            "primary_metric": args.report_primary_metric,
            "canonical_metric_mode": args.canonical_metric_mode,
            "canonical_metrics_available": canonical_report is not None,
            "protocol": protocol,
        },
    }
    if canonical_report is not None:
        eval_summary["metrics"]["auxiliary_metrics"] = {
            "canonicalized": {
                "strict_precision": round(canonical_report.strict_precision, 4),
                "strict_recall": round(canonical_report.strict_recall, 4),
                "strict_f1": round(canonical_report.strict_f1, 4),
                "relaxed_precision": round(canonical_report.relaxed_precision, 4),
                "relaxed_recall": round(canonical_report.relaxed_recall, 4),
                "relaxed_f1": round(canonical_report.relaxed_f1, 4),
                "type_precision": round(canonical_report.type_precision, 4),
                "type_recall": round(canonical_report.type_recall, 4),
                "type_f1": round(canonical_report.type_f1, 4),
                "schema_compliance_rate": round(canonical_report.schema_compliance_rate, 4),
                "canonical_role_rewrites_total": canonical_rewrites_total,
                "canonical_role_rewrites_avg": round(
                    canonical_rewrites_total / report.total_samples if report.total_samples else 0.0,
                    4,
                ),
            }
        }
    save_json(summary_file, eval_summary)

    run_manifest = build_run_manifest(
        task="eval_local",
        status="completed",
        meta=eval_summary["meta"],
        artifacts={
            "run_dir": os.path.abspath(artifact_dir),
            "result_file": os.path.abspath(final_output_path),
            "metrics_file": os.path.abspath(metrics_file),
            "summary_file": os.path.abspath(summary_file),
        },
        runtime=eval_summary["runtime"],
        runtime_manifest=runtime_manifest,
    )
    save_json(run_manifest_path, run_manifest)

    print(f"   ç»“æœæ–‡ä»¶: {final_output_path}")
    print(f"   æŒ‡æ ‡æ–‡ä»¶: {metrics_file}")
    print(f"   æ‘˜è¦æ–‡ä»¶: {summary_file}")
    print(f"   è¿è¡Œæ¸…å•: {run_manifest_path}")
    print("\nâœ… è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
